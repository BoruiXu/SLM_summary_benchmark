from lm_eval.api.task import ConfigurableTask
from lm_eval.api.instance import Instance
# from lm_eval.api.registry import register_task
from lm_eval.api.metrics import mean
import re
import torch
import sacrebleu
from rouge_score import rouge_scorer, scoring


# @register_task("cnndm_qwen_no_limit_len")
class cnndm_qwen_no_limit_len(ConfigurableTask):
    VERSION = 0
    # tmp_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen72b_summary/cnndm_sample_500_0k5_1k5_qwen_summary_no_len_limit.jsonl'
    tmp_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen72b_summary/cnndm_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl'
    def __init__(self):
        super().__init__(config={'dataset_path':'json','dataset_kwargs':{"data_files":{'train':self.tmp_path,'validation':self.tmp_path,'test':self.tmp_path}},
                                 'generation_kwargs': {'do_sample': False, 'temperature': 0.0, 'until': ['\n', '\n\n']}, 
                                 'metadata': {'version': self.VERSION}})
        self.factkb_tokenizer = None
        self.factkb_model = None
        self.bert_score = None
        print('In summary bench: cnndm qwen task!')

    def maybe_init_factkb(self):
        if self.factkb_tokenizer is None or self.factkb_model is None:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            self.factkb_tokenizer = AutoTokenizer.from_pretrained("roberta-base", padding="max_length", truncation=True)
            self.factkb_model = AutoModelForSequenceClassification.from_pretrained("bunsenfeng/FactKB", num_labels=2, device_map="auto")

    def maybe_init_bertscore(self):
        if self.bert_score is None:
            from evaluate import load
            self.bert_score = load("bertscore")

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        return f'News: {doc["article"]}\nSummarize the news in two sentences. Summary:'
        
    @staticmethod
    def should_decontaminate():
        return True

    def doc_to_decontamination_query(self, doc):
        return doc["article"]

    def doc_to_target(self, doc):
        return doc["qwen_reference_summary"]

    def construct_requests(self, doc, ctx, **kwargs):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """

        return [
            Instance(
                request_type="generate_until",
                doc=doc,
                # arguments=(ctx, {"until": ["\n", "."]}),
                arguments=(ctx, {"until": ["\n"]}), #\n"<eos>"
                # arguments=(ctx, {"until": ["<eos>"]}),
                idx=0,
                **kwargs
            )
        ]
    
    def clean_summary(self, completion):
        completion = completion[2:] if completion.startswith("\n\n") else completion
        completion = completion.replace('The news summary is: \"', '')
        completion_list = completion.split('\n')
        if(len(completion_list)<3):
            completion = completion_list[-1]
        else:
            completion = completion.split('\n')[2]
        candidate = completion
        complete_sentences = re.findall(r'[^.!?]*[.!?]', completion)
        # 将匹配到的完整句子连接成一个新的字符串
        completion = ''.join(complete_sentences)
        if(completion==''):
            completion = candidate
        return completion
    
    def apply_filters(self):
        
        resps, docs = zip(*((inst.resps, inst.doc) for inst in self._instances))
        resps, docs = list(resps), list(docs)
        
        for i in range(len(resps)):
            resps[i] = self.clean_summary(resps[i][0])
            
            
        for inst, resp in zip(self._instances, resps):
            inst.filtered_resps['cleaning_summary'] = resp
            
    def process_results(self, doc, results):
        completion = results[0]
        #process result
        # completion = self.clean_summary(completion)
        # results[0] = completion    
        
        # breakpoint()

        document = doc["article"]
        gold_summary = doc["qwen_reference_summary"]

        true_refs = [doc["qwen_reference_summary"]]
        all_refs = true_refs
        # self.maybe_init_factkb()
        # input_factkb = [[completion, document]]
        # factkb_tokens = self.factkb_tokenizer(input_factkb, return_tensors="pt", padding="max_length", truncation=True).to(self.factkb_model.device)
        # factkb_logits = self.factkb_model(**factkb_tokens).logits
        # factkb_res = torch.softmax(factkb_logits, dim=1)

        self.maybe_init_bertscore()
        bert_score_res = self.bert_score.compute(predictions=[completion], references=[gold_summary], model_type="microsoft/deberta-xlarge-mnli", lang="en")

        res = {
            # "factKB": float(factkb_res[0][1])*100,
            "bertscore_f1": float(bert_score_res["f1"][0])*100,
        }

        # breakpoint()

        return res

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {k: mean for k in ["bertscore_f1"]}

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {k: True for k in ["bertscore_f1"]}
