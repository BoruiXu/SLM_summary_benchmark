{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting stars in the news.\n",
    "#need function to add stars\n",
    "#need prompt\n",
    "#need function to generate stars\n",
    "#need to get the correlation with the human annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "import re\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from openai import OpenAI          \n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8880/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star list: [10  6]\n",
      "News: This a test example. I said \"who are you?\" I am Nick, my name is jack. The little penguin counted 10 ★. Who are you? I want to eat some food, I am hungry. But I want to drink water! HHHHHH? The little penguin counted 6 ★.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4cbb8c1a4c458991dc417ca79b4156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Little Penguin\": [10, 6]\n",
      "}\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "System: You are an AI assistant. You will be given a task. You must generate a detailed and\n"
     ]
    }
   ],
   "source": [
    "#insert according to the average word number\n",
    "def insert_star(text,star_list,star_number):\n",
    "    words_list = re.findall(r'\\S+|\\n', text)\n",
    "    # from nltk import word_tokenize\n",
    "    # words_list = word_tokenize(text)\n",
    "    interval = len(words_list) // star_number\n",
    "    res = ''  \n",
    "    for i in range(star_number):\n",
    "        current_star = star_list[i]\n",
    "        single_star = f\"The little penguin counted {current_star} ★. \"\n",
    "        st = i*interval\n",
    "        ed = (i+1)*interval\n",
    "        if(i==star_number-1):\n",
    "            ed = len(words_list)\n",
    "        for j in range(st,ed):\n",
    "            if(words_list[j] == '\\n'):\n",
    "                res = res.strip()\n",
    "                res += words_list[j]\n",
    "            else:\n",
    "                res += words_list[j] + ' '\n",
    "        res += single_star \n",
    "    res = res.strip()\n",
    "    return res\n",
    "\n",
    "def insert_star_after_sentence(text,star_list,star_number):\n",
    "    from nltk.tokenize import sent_tokenize  \n",
    "    sentences_list = sent_tokenize(text)\n",
    "    # sentences_list = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    interval = len(sentences_list) // star_number\n",
    "    if(interval == 0):\n",
    "        interval = 1\n",
    "        star_number = len(sentences_list)\n",
    "    res = ''\n",
    "    for i in range(star_number):\n",
    "        current_star = star_list[i]\n",
    "        single_star = f\"The little penguin counted {current_star} ★. \"\n",
    "        st = i*interval\n",
    "        ed = (i+1)*interval\n",
    "        if(i==star_number-1):\n",
    "            ed = len(sentences_list)\n",
    "        for j in range(st,ed):\n",
    "            res += sentences_list[j] + ' '\n",
    "        res += single_star\n",
    "    res = res.strip()\n",
    "    return res\n",
    "            \n",
    "\n",
    "def generate_star_list(star_number):\n",
    "    return np.random.randint(1,20,star_number)\n",
    "\n",
    "def LLM_count(path,star_number=2,llm_model=\"Qwen/Qwen1.5-7B\",mode=\"sentence\"):\n",
    "    #temporal test\n",
    "    test = \"News: This a test example. I said \\\"who are you?\\\" I am Nick, my name is jack.\"\\\n",
    "            \" Who are you? I want to eat some food, I am hungry. But I want to drink water! HHHHHH?\"\n",
    "    \n",
    "    prompt = 'The little penguin is counting ★ in the news. '\\\n",
    "            'Please help little penguin collect all the numbers of ★, for example: {\\\"Little Penguin\\\":[x,x,x,...]}, no sum is required, '\\\n",
    "            'the numbers in [x,x,x,...] are the number of ★ the penguin counted each time. Please only output in JSON format without any explanation.'        \n",
    "    \n",
    "    star_list = generate_star_list(star_number)        \n",
    "    print(f'star list: {star_list}')\n",
    "    \n",
    "    input_text = insert_star_after_sentence(test,star_list,star_number)\n",
    "    # input_text = prompt+'\\n'+input_text+'\\nResult: '  \n",
    "    print(input_text)        \n",
    "    model = AutoModelForCausalLM.from_pretrained(llm_model, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model, trust_remote_code=True)\n",
    "    with torch.no_grad():\n",
    "        # l = len(input_text)\n",
    "        # input_text = tokenizer(input_text, return_tensors=\"pt\",return_attention_mask=False)\n",
    "        # output = model.generate(**input_text, max_new_tokens=20)\n",
    "        # print(tokenizer.batch_decode(output)[0][l:])\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": input_text+'\\nResult: '}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        print(response)\n",
    "    \n",
    "    #chat model\n",
    "    # chat_response = client.chat.completions.create(\n",
    "    #     model=\"01-ai/Yi-34B-Chat\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": prompt},\n",
    "    #         {\"role\": \"user\", \"content\": \"News: \"+input_text+'\\nResult: '},\n",
    "            \n",
    "    #     ],\n",
    "    #     temperature=0,\n",
    "    #     max_tokens=50,\n",
    "    # )\n",
    "    \n",
    "    # res = chat_response.choices[0].message.content\n",
    "    # print(res)    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "LLM_count(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个示例文本. 这是第二个句子. 这是第三个句子.*** 这是第四个句子. 这是第五个句子.*** 这是第六个句子。\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
