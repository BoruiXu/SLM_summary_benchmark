{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForSeq2SeqLM\n",
    "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "# from transformers import AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from evaluate import load\n",
    "bert_score = load(\"bertscore\")\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[refs], references=[preds], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res\n",
    "\n",
    "def get_score(refs, preds):\n",
    "    rouge_res = rouge(refs, preds)\n",
    "    \n",
    "    bert_score = 0\n",
    "    for i in range(len(refs)): \n",
    "        bert_score += BertScore(refs[i], preds[i])[\"f1\"][0]\n",
    "    \n",
    "    total_res = {\n",
    "            \"rouge1\": rouge_res[\"rouge1\"],\n",
    "            \"rougeL\": rouge_res[\"rougeLsum\"],\n",
    "            \"bertscore_f1\": bert_score/len(refs)\n",
    "        }\n",
    "    \n",
    "    return total_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.36650528185885634, 'rougeL': 0.2513014265393698, 'bertscore_f1': 0.6738523904766355}\n",
      "{'rouge1': 0.3747219478308891, 'rougeL': 0.24548436002357307, 'bertscore_f1': 0.6767936660242933}\n",
      "{'rouge1': 0.45431033160176304, 'rougeL': 0.33080460741759454, 'bertscore_f1': 0.7225180704678807}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(\"/home/xbr/LLM/benchmark_llm_summarization/pair_with_qwen.json\")\n",
    "\n",
    "print(data.shape)\n",
    "writer = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['writer_summary'].to_list()\n",
    "davinci = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['text-davinci-002_summary'].to_list()\n",
    "qwen = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['qwen_summary'].to_list()\n",
    "print(get_score(writer,davinci))\n",
    "print(get_score(writer,qwen))\n",
    "print(get_score(qwen,davinci))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1, qwen score 4.77, llama score 3.26, human scoe 3.219999999999999.\n",
      "M10, qwen score 3.98, llama score 3.07, human scoe 2.7266666666666657.\n",
      "M11, qwen score 3.46, llama score 2.99, human scoe 2.2799999999999994.\n",
      "M12, qwen score 4.77, llama score 3.28, human scoe 3.5966666666666667.\n",
      "M13, qwen score 4.7, llama score 3.18, human scoe 3.4433333333333334.\n",
      "M14, qwen score 4.61, llama score 3.12, human scoe 3.1966666666666668.\n",
      "M15, qwen score 4.62, llama score 3.21, human scoe 3.3466666666666653.\n",
      "M17, qwen score 4.9, llama score 3.36, human scoe 3.996666666666667.\n",
      "M2, qwen score 4.89, llama score 3.27, human scoe 3.2766666666666664.\n",
      "M20, qwen score 3.79, llama score 3.18, human scoe 3.6333333333333333.\n",
      "M22, qwen score 4.92, llama score 3.26, human scoe 4.18.\n",
      "M23, qwen score 4.83, llama score 3.33, human scoe 4.163333333333333.\n",
      "M5, qwen score 4.98, llama score 3.47, human scoe 3.71.\n",
      "M8, qwen score 4.54, llama score 3.18, human scoe 3.2900000000000005.\n",
      "M9, qwen score 4.51, llama score 3.05, human scoe 2.383333333333333.\n"
     ]
    }
   ],
   "source": [
    "#ger correlation\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "import numpy as np\n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "import json\n",
    "qwen_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/qwen/Qwen1.5-72B-Chat_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "llama_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/llama2_70b/Llama-2-70b-chat-hf_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "human_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/qwen/human_score_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "new_dict = {}\n",
    "for i in qwen_eva.keys():\n",
    "    # tmp_list = []\n",
    "    # for j in range(len(qwen_eva[i])):\n",
    "    #     tmp_list.append((qwen_eva[i][j]+llama_eva[i][j])/2)\n",
    "    # new_dict[i] = tmp_list\n",
    "    \n",
    "    print(f'{i}, qwen score {np.mean(qwen_eva[i])}, llama score {np.mean(llama_eva[i])}, human scoe {np.mean(human_eva[i])}.')\n",
    "\n",
    "# correlation_score(qwen_eva, human_eva)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating dataset:  filter_annotations_summeval_reference\n",
      "evaluating model:  M1\n",
      "evaluating model:  M10\n",
      "evaluating model:  M11\n",
      "evaluating model:  M12\n",
      "evaluating model:  M13\n",
      "evaluating model:  M14\n",
      "evaluating model:  M15\n",
      "evaluating model:  M17\n",
      "evaluating model:  M2\n",
      "evaluating model:  M20\n",
      "evaluating model:  M22\n",
      "evaluating model:  M23\n",
      "evaluating model:  M5\n",
      "evaluating model:  M8\n",
      "evaluating model:  M9\n",
      "kendalltau correlation of system level is  0.12380952380952381\n",
      "spearmans correlation of system level is  0.1607142857142857\n",
      "kendalltau correlation of summary level is  0.13073467435394007\n",
      "spearmans correlation of summary level is  0.18163187689654095\n"
     ]
    }
   ],
   "source": [
    "#this noetbook focus on the evaluation of summary quality using rouge and bert score\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from openai import OpenAI\n",
    "import sacrebleu\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "# from evaluate import load\n",
    "# bert_score = load(\"bertscore\")\n",
    "\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge4\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "# def BertScore(refs, preds):\n",
    "#     bert_score_res = bert_score.compute(predictions=[refs], references=[preds], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "#     return bert_score_res\n",
    "\n",
    "def get_score(refs, preds):\n",
    "    rouge_res = rouge([refs], [preds])\n",
    "    bert_score = 0  #BertScore(refs, preds)[\"f1\"][0]\n",
    "    \n",
    "    # chrf = sacrebleu.corpus_chrf(preds, refs).score\n",
    "    total_res = {\n",
    "            \"rouge1\": rouge_res[\"rouge1\"],\n",
    "            \"rougeL\": rouge_res[\"rougeLsum\"],\n",
    "            \"bertscore_f1\": bert_score,\n",
    "            # \"chrf\": chrf\n",
    "        }\n",
    "    \n",
    "    return total_res\n",
    "\n",
    "\n",
    "def loop_score_api_chat(summary_list, reference_list, metric):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(summary_list)):\n",
    "        #if reference_list[0] is a list\n",
    "        tmp_score = 0\n",
    "        for j in range(len(reference_list[i])):\n",
    "            reference = reference_list[i][j]\n",
    "            summary = summary_list[i]\n",
    "            tmp_score +=get_score(reference, summary)[metric]\n",
    "        \n",
    "        score_list.append(tmp_score/len(reference_list[i]))\n",
    "        # score_list.append(get_score(reference_list[i], summary_list[i])[metric])\n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(path, aspect, metric = \"rougeL\"):\n",
    "    \n",
    "    dataset_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"evaluating dataset: \", dataset_name)\n",
    "    model_name = metric\n",
    "    \n",
    "    target_dataset = pd.read_json(path)\n",
    "    model_list = list(set(target_dataset['model'].tolist()))\n",
    "    model_list.remove('M0')\n",
    "    model_list = sorted(model_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #save result    \n",
    "    model_eva_dict= {}\n",
    "    human_eva_dict = {}\n",
    "\n",
    "    for m in model_list:\n",
    "        print(\"evaluating model: \", m)\n",
    "        tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "       \n",
    "        tmp_news_list = tmp_dataset['article'].tolist()\n",
    "        \n",
    "        tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "        tmp_score_list = tmp_dataset[aspect].tolist()\n",
    "        \n",
    "        tmp_reference_list = tmp_dataset['references'].tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        score_list = loop_score_api_chat(tmp_summary_list, tmp_reference_list, metric)\n",
    "        \n",
    "        model_eva_dict[m] = score_list\n",
    "        human_eva_dict[m] = tmp_score_list  \n",
    "    \n",
    "    #save the result\n",
    "    \n",
    "    # save_name = str(model_name)+'_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "    # human_save = 'human_score_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "   \n",
    "    \n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+save_name, 'w') as fp:\n",
    "    #     json.dump(model_eva_dict, fp)\n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+human_save, 'w') as fp:\n",
    "    #     json.dump(human_eva_dict, fp)\n",
    "    \n",
    "\n",
    "    correlation_score(model_eva_dict, human_eva_dict)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    p = './filter_annotations_summeval_reference.jsonl'# #'/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json'\n",
    "    aspect = \"expert_coherence\"\n",
    "    evaluate(p, aspect,metric=\"rougeL\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating dataset:  filter_annotations_summeval_llama2_summary\n",
      "evaluating model:  M1\n",
      "evaluating model:  M10\n",
      "evaluating model:  M11\n",
      "evaluating model:  M12\n",
      "evaluating model:  M13\n",
      "evaluating model:  M14\n",
      "evaluating model:  M15\n",
      "evaluating model:  M17\n",
      "evaluating model:  M2\n",
      "evaluating model:  M20\n",
      "evaluating model:  M22\n",
      "evaluating model:  M23\n"
     ]
    }
   ],
   "source": [
    "#换一种计算rouge和human相关系数的方式\n",
    "#计算qwen生成的summary的gouge和human的相关系数\n",
    "\n",
    "#this noetbook focus on the evaluation of summary quality using rouge and bert score\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from openai import OpenAI\n",
    "import sacrebleu\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from evaluate import load\n",
    "bert_score = load(\"bertscore\")\n",
    "\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge4\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[refs], references=[preds], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res\n",
    "\n",
    "def bleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style BLEU scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41\n",
    "\n",
    "    :param refs:\n",
    "        A `list` of `list` of reference `str`s.\n",
    "    :param preds:\n",
    "        A `list` of predicted `str`s.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.corpus_bleu(preds, refs, smooth_method=\"exp\", smooth_value=0.0, force=False,\n",
    "                                  lowercase=False, tokenize=\"intl\", use_effective_order=False).score\n",
    "    return score\n",
    "\n",
    "def get_score(refs, preds,metric):\n",
    "    \n",
    "    result = 0\n",
    "    if(metric[:5]==\"rouge\"):\n",
    "        rouge_res = rouge([refs], [preds])\n",
    "        result = rouge_res[metric]\n",
    "    elif(metric==\"bertscore\"):\n",
    "        result = BertScore(refs, preds)[\"f1\"][0]\n",
    "    elif(metric==\"bleu\"):\n",
    "        result = bleu([refs], [preds])\n",
    "    elif(metric==\"chrf\"):\n",
    "        if(preds==\"\"):\n",
    "            preds = \" \"\n",
    "        result = sacrebleu.corpus_chrf(preds, [refs]).score\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def loop_score_api_chat(summary_list, reference_list, metric):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    for i in range(len(summary_list)):\n",
    "            reference = reference_list[i]\n",
    "            summary = summary_list[i]\n",
    "            score_list.append(get_score(reference, summary, metric))\n",
    "    \n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "        \n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(path, aspect, metric = \"rougeLsum\", reference_model = 'reference', few_shot = 0):\n",
    "    \n",
    "    dataset_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"evaluating dataset: \", dataset_name)\n",
    "    model_name = metric\n",
    "    \n",
    "    target_dataset = pd.read_json(path)\n",
    "    model_list = list(set(target_dataset['model'].tolist()))\n",
    "    model_list.remove(reference_model)\n",
    "    model_list = sorted(model_list)\n",
    "    \n",
    "    reference = target_dataset[target_dataset[\"model\"]==reference_model]\n",
    "    \n",
    "    \n",
    "    #save result    \n",
    "    model_eva_dict= {}\n",
    "    human_eva_dict = {}\n",
    "\n",
    "    for m in model_list:\n",
    "        print(\"evaluating model: \", m)\n",
    "        tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "        \n",
    "        tmp_news_list = tmp_dataset['article'].tolist()\n",
    "        tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "        tmp_score_list = tmp_dataset[aspect].tolist()\n",
    "        \n",
    "        tmp_reference_list = tmp_dataset['qwen_summary'].tolist()\n",
    "        \n",
    "        \n",
    "        # for i in range(len(tmp_news_list)):\n",
    "        #     tmp_reference_list.append(reference[reference['article']==tmp_news_list[i]]['summary'].values[0])\n",
    "        \n",
    "        \n",
    "        score_list = loop_score_api_chat(tmp_summary_list, tmp_reference_list, metric)\n",
    "        \n",
    "        model_eva_dict[m] = score_list\n",
    "        human_eva_dict[m] = tmp_score_list  \n",
    "    \n",
    "    #save the result\n",
    "    \n",
    "    save_name = str(model_name)+'_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "    human_save = 'human_score_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "   \n",
    "    \n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+save_name, 'w') as fp:\n",
    "    #     json.dump(model_eva_dict, fp)\n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+human_save, 'w') as fp:\n",
    "    #     json.dump(human_eva_dict, fp)\n",
    "    \n",
    "\n",
    "    correlation_score(model_eva_dict, human_eva_dict)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    p = './filter_annotations_summeval_llama2_summary.jsonl'#'/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_xsum_average_with_qwen.json'#'./filter_annotations_summeval.jsonl'# #\n",
    "    aspect = \"expert_coherence\"\n",
    "    evaluate(p, aspect,reference_model=\"M0\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.5270462766947298, pvalue=0.206507295485425)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
