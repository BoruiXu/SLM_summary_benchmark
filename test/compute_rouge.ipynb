{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModelForSeq2SeqLM\n",
    "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "# from transformers import AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from evaluate import load\n",
    "bert_score = load(\"bertscore\")\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[refs], references=[preds], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res\n",
    "\n",
    "def get_score(refs, preds):\n",
    "    rouge_res = rouge(refs, preds)\n",
    "    \n",
    "    bert_score = 0\n",
    "    for i in range(len(refs)): \n",
    "        bert_score += BertScore(refs[i], preds[i])[\"f1\"][0]\n",
    "    \n",
    "    total_res = {\n",
    "            \"rouge1\": rouge_res[\"rouge1\"],\n",
    "            \"rougeL\": rouge_res[\"rougeLsum\"],\n",
    "            \"bertscore_f1\": bert_score/len(refs)\n",
    "        }\n",
    "    \n",
    "    return total_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.36650528185885634, 'rougeL': 0.2513014265393698, 'bertscore_f1': 0.6738523904766355}\n",
      "{'rouge1': 0.3747219478308891, 'rougeL': 0.24548436002357307, 'bertscore_f1': 0.6767936660242933}\n",
      "{'rouge1': 0.45431033160176304, 'rougeL': 0.33080460741759454, 'bertscore_f1': 0.7225180704678807}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(\"/home/xbr/LLM/benchmark_llm_summarization/pair_with_qwen.json\")\n",
    "\n",
    "print(data.shape)\n",
    "writer = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['writer_summary'].to_list()\n",
    "davinci = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['text-davinci-002_summary'].to_list()\n",
    "qwen = data.drop_duplicates(subset=[\"article_id\",\"writer_id\"])['qwen_summary'].to_list()\n",
    "print(get_score(writer,davinci))\n",
    "print(get_score(writer,qwen))\n",
    "print(get_score(qwen,davinci))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1, qwen score 4.77, llama score 3.26, human scoe 3.219999999999999.\n",
      "M10, qwen score 3.98, llama score 3.07, human scoe 2.7266666666666657.\n",
      "M11, qwen score 3.46, llama score 2.99, human scoe 2.2799999999999994.\n",
      "M12, qwen score 4.77, llama score 3.28, human scoe 3.5966666666666667.\n",
      "M13, qwen score 4.7, llama score 3.18, human scoe 3.4433333333333334.\n",
      "M14, qwen score 4.61, llama score 3.12, human scoe 3.1966666666666668.\n",
      "M15, qwen score 4.62, llama score 3.21, human scoe 3.3466666666666653.\n",
      "M17, qwen score 4.9, llama score 3.36, human scoe 3.996666666666667.\n",
      "M2, qwen score 4.89, llama score 3.27, human scoe 3.2766666666666664.\n",
      "M20, qwen score 3.79, llama score 3.18, human scoe 3.6333333333333333.\n",
      "M22, qwen score 4.92, llama score 3.26, human scoe 4.18.\n",
      "M23, qwen score 4.83, llama score 3.33, human scoe 4.163333333333333.\n",
      "M5, qwen score 4.98, llama score 3.47, human scoe 3.71.\n",
      "M8, qwen score 4.54, llama score 3.18, human scoe 3.2900000000000005.\n",
      "M9, qwen score 4.51, llama score 3.05, human scoe 2.383333333333333.\n"
     ]
    }
   ],
   "source": [
    "#ger correlation\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "import numpy as np\n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "import json\n",
    "qwen_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/qwen/Qwen1.5-72B-Chat_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "llama_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/llama2_70b/Llama-2-70b-chat-hf_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "human_eva = json.load(open(\"./LLM_evaluation_correlation_with_human/qwen/human_score_filter_annotations_summeval_expert_coherence_eva.json\"))\n",
    "\n",
    "new_dict = {}\n",
    "for i in qwen_eva.keys():\n",
    "    # tmp_list = []\n",
    "    # for j in range(len(qwen_eva[i])):\n",
    "    #     tmp_list.append((qwen_eva[i][j]+llama_eva[i][j])/2)\n",
    "    # new_dict[i] = tmp_list\n",
    "    \n",
    "    print(f'{i}, qwen score {np.mean(qwen_eva[i])}, llama score {np.mean(llama_eva[i])}, human scoe {np.mean(human_eva[i])}.')\n",
    "\n",
    "# correlation_score(qwen_eva, human_eva)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating dataset:  filter_annotations_summeval_reference\n",
      "evaluating model:  M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model:  M10\n",
      "evaluating model:  M11\n",
      "evaluating model:  M12\n",
      "evaluating model:  M13\n",
      "evaluating model:  M14\n",
      "evaluating model:  M15\n",
      "evaluating model:  M17\n",
      "evaluating model:  M2\n",
      "evaluating model:  M20\n",
      "evaluating model:  M22\n",
      "evaluating model:  M23\n",
      "evaluating model:  M5\n",
      "evaluating model:  M8\n",
      "evaluating model:  M9\n",
      "kendalltau correlation of system level is  0.4285714285714286\n",
      "spearmans correlation of system level is  0.5464285714285714\n",
      "kendalltau correlation of summary level is  0.2691282866520724\n",
      "spearmans correlation of summary level is  0.37271425757591703\n"
     ]
    }
   ],
   "source": [
    "#this noetbook focus on the evaluation of summary quality using rouge and bert score\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from openai import OpenAI\n",
    "import sacrebleu\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from evaluate import load\n",
    "\n",
    "bert_score = load(\"bertscore\")\n",
    "# bleurt = load(\"bleurt\",\"BLEURT-20\")\n",
    "meteor = load(\"meteor\")\n",
    "rouge_hf = load('rouge')\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[preds], references=[refs], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res\n",
    "\n",
    "# def BLEURT(refs, preds):\n",
    "#     bleurt_res = bleurt.compute(predictions=[refs], references=[preds])\n",
    "#     return bleurt_res\n",
    "\n",
    "def bleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style BLEU scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41\n",
    "\n",
    "    :param refs:\n",
    "        A `list` of `list` of reference `str`s.\n",
    "    :param preds:\n",
    "        A `list` of predicted `str`s.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.corpus_bleu(preds, refs, smooth_method=\"exp\", smooth_value=0.0, force=False,\n",
    "                                  lowercase=False, tokenize=\"intl\", use_effective_order=False).score\n",
    "    return score\n",
    "\n",
    "def get_score(refs, preds,metric):\n",
    "    if(preds==\"\"):\n",
    "        preds = \" \"\n",
    "    result = 0\n",
    "    if(metric[:5]==\"rouge\" and metric!=\"rouge_hf\"):\n",
    "        rouge_res = rouge([refs], [preds])\n",
    "        result = rouge_res[metric]\n",
    "    elif(metric==\"bertscore\"):\n",
    "        result = BertScore(refs, preds)[\"f1\"][0]\n",
    "    elif(metric==\"bleu\"):\n",
    "        result = bleu([refs], [preds])\n",
    "    elif(metric==\"chrf\"):\n",
    "        result = sacrebleu.corpus_chrf(preds, [refs]).score\n",
    "    # elif(metric==\"bleurt\"):\n",
    "    #     result = BLEURT(refs, preds)[\"scores\"][0]\n",
    "    elif(metric==\"meteor\"):\n",
    "        result = meteor.compute(predictions=[preds], references=[refs])[\"meteor\"]\n",
    "    elif(metric==\"rouge_hf\"):\n",
    "        result = rouge_hf.compute(predictions=[preds], references=[refs])[\"rougeLsum\"]\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def loop_score_api_chat(summary_list, reference_list, metric):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(summary_list)):\n",
    "        #if reference_list[0] is a list\n",
    "        tmp_score = 0\n",
    "        for j in range(len(reference_list[i])):\n",
    "            reference = reference_list[i][j]\n",
    "            summary = summary_list[i]\n",
    "            tmp_score +=get_score(reference, summary, metric)\n",
    "        \n",
    "        score_list.append(tmp_score/len(reference_list[i]))\n",
    "        # score_list.append(get_score(reference_list[i], summary_list[i])[metric])\n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(path, aspect, metric = \"rougeL\"):\n",
    "    \n",
    "    dataset_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"evaluating dataset: \", dataset_name)\n",
    "    model_name = metric\n",
    "    \n",
    "    target_dataset = pd.read_json(path)\n",
    "    model_list = list(set(target_dataset['model'].tolist()))\n",
    "    model_list.remove('M0')\n",
    "    model_list = sorted(model_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #save result    \n",
    "    model_eva_dict= {}\n",
    "    human_eva_dict = {}\n",
    "\n",
    "    for m in model_list:\n",
    "        print(\"evaluating model: \", m)\n",
    "        tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "       \n",
    "        tmp_news_list = tmp_dataset['article'].tolist()\n",
    "        \n",
    "        tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "        tmp_score_list = tmp_dataset[aspect].tolist()\n",
    "        \n",
    "        tmp_reference_list = tmp_dataset['references'].tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "        score_list = loop_score_api_chat(tmp_summary_list, tmp_reference_list, metric)\n",
    "        \n",
    "        model_eva_dict[m] = score_list\n",
    "        human_eva_dict[m] = tmp_score_list  \n",
    "    \n",
    "    #save the result\n",
    "    \n",
    "    save_name = str(model_name)+'_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "    human_save = 'human_score_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "   \n",
    "    \n",
    "    with open('./LLM_evaluation_correlation_with_human/'+save_name, 'w') as fp:\n",
    "        json.dump(model_eva_dict, fp)\n",
    "    with open('./LLM_evaluation_correlation_with_human/'+human_save, 'w') as fp:\n",
    "        json.dump(human_eva_dict, fp)\n",
    "    \n",
    "\n",
    "    correlation_score(model_eva_dict, human_eva_dict)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    p = './filter_annotations_summeval_reference.jsonl'# #'/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json'\n",
    "    aspect = \"expert_coherence\"\n",
    "    evaluate(p, aspect,metric=\"bertscore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xbr/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating dataset:  likert_evaluation_results_cnndm_average_with_llama2\n",
      "evaluating model:  anthropic_stanford-online-all-v4-s3\n",
      "Thai Smile, a subsidiary of Thai Airways, unveils new livery featuring Jake, Finn and Princess Bubblegum . The interior of the plane also has an Adventure Time theme . The inaugural Thai Smile Adventure Time flight takes place on April 4, heading from Bangkok to Phuket .\n",
      "********************\n",
      "  Thai Smile, a subsidiary of Thai Airways, has unveiled a new Airbus A320 with a livery featuring characters from the popular Cartoon Network show \"Adventure Time.\" The plane's interior is also themed around the show, with overhead bins, head rests, and air sickness bags featuring the faces of characters like Jake, Finn, and Princess Bubblegum.\n",
      "0.35133589161532086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m# p = '/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_xsum_average.json'#'./filter_annotations_summeval.jsonl'#'./filter_annotations_summeval.jsonl'# #\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=194'>195</a>\u001b[0m aspect \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcoherence\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m evaluate(p, aspect,metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmeteor\u001b[39;49m\u001b[39m'\u001b[39;49m,reference_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mreference\u001b[39;49m\u001b[39m'\u001b[39;49m,llm\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39m# p = './filter_annotations_summeval.jsonl'#'./filter_annotations_summeval.jsonl'\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=197'>198</a>\u001b[0m \u001b[39m# aspect = \"expert_relevance\"\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39m# evaluate(p, aspect,metric=\"bertscore\", reference_model = 'M0')\u001b[39;00m\n",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tmp_news_list)):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m         tmp_reference_list\u001b[39m.\u001b[39mappend(reference[reference[\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mtmp_news_list[i]][\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m score_list \u001b[39m=\u001b[39m loop_score_api_chat(tmp_summary_list, tmp_reference_list, metric)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m model_eva_dict[m] \u001b[39m=\u001b[39m score_list\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m human_eva_dict[m] \u001b[39m=\u001b[39m tmp_score_list  \n",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m         reference \u001b[39m=\u001b[39m reference_list[i]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m         summary \u001b[39m=\u001b[39m summary_list[i]\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m         score_list\u001b[39m.\u001b[39mappend(get_score(reference, summary, metric))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score_list\n",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb 单元格 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     result \u001b[39m=\u001b[39m meteor\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39m[preds], references\u001b[39m=\u001b[39m[refs\u001b[39m.\u001b[39mstrip()[:\u001b[39mlen\u001b[39m(preds)]])[\u001b[39m\"\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39mprint\u001b[39m(result)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m         a \u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/compute_rouge.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39melif\u001b[39;00m(metric\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrouge_hf\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this noetbook focus on the evaluation of summary quality using rouge and bert score\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from openai import OpenAI\n",
    "import sacrebleu\n",
    "#load bert score model\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "from evaluate import load\n",
    "\n",
    "bert_score = load(\"bertscore\")\n",
    "# bleurt = load(\"bleurt\",\"BLEURT-20\")\n",
    "meteor = load(\"meteor\")\n",
    "rouge_hf = load('rouge')\n",
    "\n",
    "\n",
    "def rouge(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style ROUGE scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L68\n",
    "    :param refs:\n",
    "        A `list` of reference `strs`.\n",
    "    :param preds:\n",
    "        A `list` of predicted `strs`.\n",
    "    \"\"\"\n",
    "    rouge_types = [\"rouge1\", \"rouge2\", \"rougeLsum\"]\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types)\n",
    "    # Add newlines between sentences to correctly compute `rougeLsum`.\n",
    "\n",
    "    def _prepare_summary(summary):\n",
    "        summary = summary.replace(\" . \", \".\\n\")\n",
    "        return summary\n",
    "\n",
    "    # Accumulate confidence intervals.\n",
    "    aggregator = scoring.BootstrapAggregator()\n",
    "    for ref, pred in zip(refs, preds):\n",
    "        ref = _prepare_summary(ref)\n",
    "        pred = _prepare_summary(pred)\n",
    "        aggregator.add_scores(scorer.score(ref, pred))\n",
    "    result = aggregator.aggregate()\n",
    "    \n",
    "    return {type: result[type].mid.fmeasure  for type in rouge_types}\n",
    "\n",
    "\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[preds], references=[refs], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res\n",
    "\n",
    "# def BLEURT(refs, preds):\n",
    "#     bleurt_res = bleurt.compute(predictions=[refs], references=[preds])\n",
    "#     return bleurt_res\n",
    "\n",
    "def bleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Returns `t5` style BLEU scores. See the related implementation:\n",
    "    https://github.com/google-research/text-to-text-transfer-transformer/blob/3d10afd51ba97ac29eb66ae701eca274488202f7/t5/evaluation/metrics.py#L41\n",
    "\n",
    "    :param refs:\n",
    "        A `list` of `list` of reference `str`s.\n",
    "    :param preds:\n",
    "        A `list` of predicted `str`s.\n",
    "    \"\"\"\n",
    "    score = sacrebleu.corpus_bleu(preds, refs, smooth_method=\"exp\", smooth_value=0.0, force=False,\n",
    "                                  lowercase=False, tokenize=\"intl\", use_effective_order=False).score\n",
    "    return score\n",
    "\n",
    "def get_score(refs, preds,metric):\n",
    "    if(preds==\"\"):\n",
    "        preds = \" \"\n",
    "    result = 0\n",
    "    if(metric[:5]==\"rouge\" and metric!=\"rouge_hf\"):\n",
    "        rouge_res = rouge([refs], [preds])\n",
    "        result = rouge_res[metric]\n",
    "    elif(metric==\"bertscore\"):\n",
    "        result = BertScore(refs, preds)[\"f1\"][0]\n",
    "    elif(metric==\"bleu\"):\n",
    "        result = bleu([refs], [preds])\n",
    "    elif(metric==\"chrf\"):\n",
    "        result = sacrebleu.corpus_chrf(preds, [refs]).score\n",
    "    # elif(metric==\"bleurt\"):\n",
    "    #     result = BLEURT(refs, preds)[\"scores\"][0]\n",
    "    elif(metric==\"meteor\"):\n",
    "        \n",
    "        result = meteor.compute(predictions=[preds], references=[refs])[\"meteor\"]\n",
    "    \n",
    "    elif(metric==\"rouge_hf\"):\n",
    "        result = rouge_hf.compute(predictions=[preds], references=[refs])[\"rougeLsum\"]\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def loop_score_api_chat(summary_list, reference_list, metric):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    for i in range(len(summary_list)):\n",
    "            reference = reference_list[i]\n",
    "            summary = summary_list[i]\n",
    "            score_list.append(get_score(reference, summary, metric))\n",
    "    \n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "        \n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(path, aspect, metric = \"rougeLsum\", reference_model = 'reference', llm = 0):\n",
    "    \n",
    "    dataset_name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "    print(\"evaluating dataset: \", dataset_name)\n",
    "    model_name = metric\n",
    "    \n",
    "    target_dataset = pd.read_json(path)\n",
    "    model_list = list(set(target_dataset['model'].tolist()))\n",
    "    model_list.remove(reference_model)\n",
    "    model_list = sorted(model_list)\n",
    "    \n",
    "    reference = target_dataset[target_dataset[\"model\"]==reference_model]\n",
    "    \n",
    "    \n",
    "    #save result    \n",
    "    model_eva_dict= {}\n",
    "    human_eva_dict = {}\n",
    "\n",
    "    for m in model_list:\n",
    "        print(\"evaluating model: \", m)\n",
    "        tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "        \n",
    "        tmp_news_list = tmp_dataset['article'].tolist()\n",
    "        tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "        tmp_score_list = tmp_dataset[aspect].tolist()\n",
    "        \n",
    "        tmp_reference_list = []\n",
    "        \n",
    "        if(llm==1):\n",
    "            tmp_reference_list = tmp_dataset['qwen_summary'].tolist()\n",
    "        else:\n",
    "            for i in range(len(tmp_news_list)):\n",
    "                tmp_reference_list.append(reference[reference['article']==tmp_news_list[i]]['summary'].values[0])\n",
    "        \n",
    "        \n",
    "        score_list = loop_score_api_chat(tmp_summary_list, tmp_reference_list, metric)\n",
    "        \n",
    "        model_eva_dict[m] = score_list\n",
    "        human_eva_dict[m] = tmp_score_list  \n",
    "    \n",
    "    #save the result\n",
    "    \n",
    "    save_name = str(model_name)+'_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "    human_save = 'human_score_'+str(dataset_name)+'_'+str(aspect)+'_eva.json'\n",
    "   \n",
    "    \n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+save_name, 'w') as fp:\n",
    "    #     json.dump(model_eva_dict, fp)\n",
    "    # with open('./LLM_evaluation_correlation_with_human/'+human_save, 'w') as fp:\n",
    "    #     json.dump(human_eva_dict, fp)\n",
    "    \n",
    "\n",
    "    correlation_score(model_eva_dict, human_eva_dict)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    p = '/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average_with_llama2.json'\n",
    "    # p = '/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_xsum_average.json'#'./filter_annotations_summeval.jsonl'#'./filter_annotations_summeval.jsonl'# #\n",
    "    aspect = \"coherence\"\n",
    "    evaluate(p, aspect,metric='meteor',reference_model='reference',llm=1)\n",
    "    # p = './filter_annotations_summeval.jsonl'#'./filter_annotations_summeval.jsonl'\n",
    "    # aspect = \"expert_relevance\"\n",
    "    # evaluate(p, aspect,metric=\"bertscore\", reference_model = 'M0')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/xbr/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/e5c65cb2c4c7b1851de53937b621367634cb04a10df99a6ba8a4aa87c740bf38/BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/xbr/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/e5c65cb2c4c7b1851de53937b621367634cb04a10df99a6ba8a4aa87c740bf38/BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/xbr/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/e5c65cb2c4c7b1851de53937b621367634cb04a10df99a6ba8a4aa87c740bf38/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: /home/xbr/.cache/huggingface/metrics/bleurt/BLEURT-20/downloads/extracted/e5c65cb2c4c7b1851de53937b621367634cb04a10df99a6ba8a4aa87c740bf38/BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [1.0149897336959839, 0.998480498790741]}\n"
     ]
    }
   ],
   "source": [
    "#bleurt\n",
    "from evaluate import load\n",
    "bleurt = load(\"bleurt\",\"BLEURT-20\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "\n",
    "results = bleurt.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.9999999999999999, pvalue=0.016666666666666666)\n"
     ]
    }
   ],
   "source": [
    "# from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "# a = [1,3,4,2,5]\n",
    "# b = [1,2,3,1.5,5]\n",
    "\n",
    "# print(kendalltau(a, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
