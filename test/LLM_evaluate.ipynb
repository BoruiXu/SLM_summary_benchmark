{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this noetbook focus on the evaluation of summary quality using LLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8880/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "llm_model = \"Qwen/Qwen1.5-72B-Chat\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function block\n",
    "\n",
    "#genreate prompt\n",
    "def generate_prompt(few_shot: int, aspect:str, refer_news = None, refer_summary = None, refer_score = None):\n",
    "    prompt = \"You are a helpful summary assistant.\"\n",
    "    if(aspect!=\"Faithfulness\"):\n",
    "        base_prompt = \"In this task, you will be provided with a news article and a generated summary.\\n\"\\\n",
    "                \"Your task is to rate the \" + aspect +\" of the generated summary with a score from 1 to 5, \"\\\n",
    "                \"where 1 is the lowest and 5 is the highest.\\n\"\\\n",
    "                \"Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\"\n",
    "        if(few_shot==0):\n",
    "            prompt = base_prompt+ \"Example response:\\n\"+aspect+\" (1-5): 3\"\n",
    "        else:\n",
    "            shot_num = len(refer_news)\n",
    "            if shot_num>2:\n",
    "                shot_num = 2\n",
    "            if(shot_num==1):\n",
    "                base_prompt+=\"This is an example:\\n\"\n",
    "            else:\n",
    "                base_prompt+=\"These are examples:\\n\"\n",
    "            \n",
    "            for i in range(shot_num):\n",
    "                base_prompt = base_prompt + \"News: \"+refer_news[i]+\"\\nGenerated summary: \"+refer_summary[i]+\"\\n\"+aspect+\" (1-5): \"+str(refer_score[i])+\"\\n\"\n",
    "            prompt = base_prompt\n",
    "    else:\n",
    "        base_prompt = \"In this task, you will be provided with a news article and a generated summary.\\n\"\\\n",
    "                \"Your task is to rate the \" + aspect +\" of the generated summary with a score 0 or 1, \"\\\n",
    "                \"where 1 is not fact and 1 is fact.\\n\"\\\n",
    "                \"Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\"\n",
    "        \n",
    "        if(few_shot==0):\n",
    "            prompt = base_prompt+ \"Example response:\\n\"+aspect+\" (0,1): 1\"\n",
    "        else:\n",
    "            shot_num = len(refer_news)\n",
    "\n",
    "            if shot_num>2:\n",
    "                shot_num = 2\n",
    "            if(shot_num==1):\n",
    "                base_prompt+=\"This is an example:\\n\"\n",
    "            else:\n",
    "                base_prompt+=\"These are examples:\\n\"\n",
    "            for i in range(shot_num):\n",
    "                base_prompt = base_prompt + \"News: \"+refer_news[i]+\"\\nGenerated summary: \"+refer_summary[i]+\"\\n\"+aspect+\" (0,1): \"+str(refer_score[i])+\"\\n\"\n",
    "            prompt = base_prompt\n",
    "                   \n",
    "    return prompt\n",
    "    \n",
    "\n",
    "#get socre using openai api\n",
    "def score_api_chat(client, model_name, prompt, user_input, temperature = 0):\n",
    "    chat_response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                \n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        # \"\\n\\nReference summary: \"\n",
    "        #         +reference_summary+\n",
    "    res = chat_response.choices[0].message.content\n",
    "        \n",
    "    match = re.search(r':\\s*(\\d+)', res)   \n",
    "    score = match.group(1) if match is not None else 1\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "def loop_score_api_chat(news_list, summary_list, client, model_name, aspect, \n",
    "                        refer_news = None, refer_summary = None, refer_score = None,temperature = 0):\n",
    "    \n",
    "    score_list = []\n",
    "    prompt = \"\"\n",
    "    if(refer_news is None):\n",
    "        prompt = generate_prompt(0, aspect)  \n",
    "    else:\n",
    "        prompt = generate_prompt(1, aspect, refer_news, refer_summary, refer_score)\n",
    "\n",
    "    \n",
    "    for i in range(len(news_list)):\n",
    "            news = news_list[i]\n",
    "            summary = summary_list[i]\n",
    "            user_input = \"News: \"+news+\"\\nGenerated summary: \"+summary\n",
    "            score_list.append(score_api_chat(client, model_name, prompt, user_input))\n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model:  together_glm\n",
      "evaluating model:  openai_text-ada-001\n",
      "evaluating model:  openai_davinci (0 shot)\n",
      "evaluating model:  together_opt-175b\n",
      "evaluating model:  openai_text-davinci-002\n",
      "evaluating model:  anthropic_stanford-online-all-v4-s3\n",
      "evaluating model:  openai_text-curie-001 (0 shot)\n",
      "evaluating model:  openai_curie\n",
      "evaluating model:  openai_text-curie-001\n",
      "evaluating model:  pegasus\n",
      "evaluating model:  cohere_xlarge-20220609\n",
      "evaluating model:  openai_ada (0 shot)\n",
      "evaluating model:  openai_curie (0 shot)\n",
      "evaluating model:  openai_text-ada-001 (0 shot)\n",
      "evaluating model:  openai_davinci\n",
      "evaluating model:  openai_ada\n",
      "evaluating model:  brio\n",
      "evaluating model:  openai_text-davinci-002 (0 shot)\n"
     ]
    }
   ],
   "source": [
    "#load the dataset to processing\n",
    "\n",
    "cnndm_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json')\n",
    "\n",
    "xsum_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_xsum_average.json')\n",
    "\n",
    "summeval = pd.read_json('./filter_annotations_.jsonl')\n",
    "\n",
    "\n",
    "target_dataset = cnndm_liker\n",
    "template_dataset = cnndm_liker\n",
    "template_index = \"None\"\n",
    "aspect = \"relevance\"\n",
    "\n",
    "tag_aspect = \"relevance\"\n",
    "\n",
    "\n",
    "model_list = list(set(target_dataset['model'].tolist()))\n",
    "\n",
    "model_list.remove('reference')\n",
    "\n",
    "model_eva_dict= {}\n",
    "human_eva_dict = {}\n",
    "# total_summary_level = 0\n",
    "\n",
    "# ['Relevance', 'Coherence', 'Consistency', 'Faithfulness', 'Informativeness']\n",
    "\n",
    "\n",
    "\n",
    "template_news = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i]['article'] for i in range(1,2)]\n",
    "template_summary = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i]['summary'] for i in range(1,2)]\n",
    "template_score = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i][aspect] for i in range(1,2)]\n",
    "\n",
    "# template_news = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i]['article'] for i in range(1)]\n",
    "# template_summary = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i]['summary'] for i in range(1)]\n",
    "# template_score = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i][tag_aspect] for i in range(1)]\n",
    "\n",
    "# template_news = None\n",
    "# template_summary = None\n",
    "# template_score = None\n",
    "\n",
    "\n",
    "\n",
    "for m in model_list:\n",
    "    print(\"evaluating model: \", m)\n",
    "    tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "    \n",
    "    tmp_news_list = tmp_dataset['article'].tolist()\n",
    "    \n",
    "    tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "    tmp_score_list = tmp_dataset[tag_aspect].tolist()\n",
    "    score_list = loop_score_api_chat(tmp_news_list, tmp_summary_list, client, llm_model, aspect, \n",
    "                                     refer_news = template_news, refer_summary = template_summary, refer_score = template_score)\n",
    "    \n",
    "    model_eva_dict[m] = score_list\n",
    "    human_eva_dict[m] = tmp_score_list\n",
    "    \n",
    "\n",
    "    \n",
    "#save the result\n",
    "# with open('./evaluate_result/'+'qwen72_summeval_'+tag_aspect+'_template_summeval_'+str(template_index)+'_eva.json', 'w') as fp:\n",
    "#     json.dump(model_eva_dict, fp)\n",
    "    \n",
    "# with open('./evaluate_result/'+'summeval_human_eva_dict.json', 'w') as fp:\n",
    "#     json.dump(human_eva_dict, fp)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation of system level is  0.7708704820679368\n",
      "spearmans correlation of system level is  0.888203040995634\n",
      "kendalltau correlation of summary level is  0.245206487822605\n",
      "spearmans correlation of summary level is  0.2796228614894566\n"
     ]
    }
   ],
   "source": [
    "correlation_score(model_eva_dict, human_eva_dict)\n",
    "# for i in model_eva_dict.keys():\n",
    "#     print(i, \":\", np.mean(model_eva_dict[i]), np.mean(human_eva_dict[i]))        \n",
    "\n",
    "#cnndm\n",
    "#index 0\n",
    "# correlation of system level is  0.813119124543455\n",
    "# correlation of summary level is  0.1472597601736428\n",
    "\n",
    "#idnex 1\n",
    "# correlation of system level is  0.7777777777777779\n",
    "# correlation of summary level is  0.20898023319686118\n",
    "\n",
    "#index 2\n",
    "# correlation of system level is  0.33333333333333337\n",
    "# correlation of summary level is  0.08772663469686875\n",
    "\n",
    "#index 4\n",
    "# correlation of system level is  0.6954252406248034\n",
    "# correlation of summary level is  nan\n",
    "\n",
    "#index None\n",
    "# correlation of system level is  0.724262721160196\n",
    "# correlation of summary level is  nan   \n",
    "\n",
    "#index 0&1\n",
    "# correlation of system level is  0.6997042637035166\n",
    "# correlation of summary level is  0.24644284212607812\n",
    "\n",
    "\n",
    "\n",
    "#xsum\n",
    "#None\n",
    "# correlation of system level is  0.7852067275033966\n",
    "# correlation of summary level is  0.24612884627714107\n",
    "\n",
    "#index 1\n",
    "# correlation of system level is  0.7158720324782436\n",
    "# correlation of summary level is  0.2570950849356606\n",
    "\n",
    "#index 2\n",
    "# correlation of system level is  0.2370435403783839\n",
    "# correlation of summary level is  0.017176762945798965\n",
    "\n",
    "#index 1 2 这个估计是2，然后是these are examples不同造成的结果不同\n",
    "# correlation of system level is  0.7306322805705787\n",
    "# correlation of summary level is  0.28464519005709527\n",
    "\n",
    "#index 3\n",
    "# correlation of system level is  0.7896732729399182\n",
    "# correlation of summary level is  0.2844474928173168\n",
    "\n",
    "# #index 2 3 \n",
    "# correlation of system level is  0.8044335210322533\n",
    "# correlation of summary level is  0.2894187475986224\n",
    "\n",
    "\n",
    "\n",
    "#summeval\n",
    "#None\n",
    "# correlation of system level is  0.8140776760988245\n",
    "# correlation of summary level is  nan\n",
    "#index 1\n",
    "# correlation of system level is  0.6315861769395361\n",
    "# correlation of summary level is  0.32975744251357864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation of system level is  0.8044335210322533\n",
      "spearmans correlation of system level is  0.9172288664537509\n",
      "kendalltau correlation of summary level is  0.2894187475986224\n",
      "spearmans correlation of summary level is  0.3382744864042585\n"
     ]
    }
   ],
   "source": [
    "dict1 = json.load(open('./evaluate_result/qwen72_xsum_relevance_template_xsum_2_3_eva.json'))\n",
    "\n",
    "dict2 = json.load(open('./evaluate_result/xsum_human_eva_dict.json'))\n",
    "\n",
    "correlation_score(dict1, dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 21, 91, 93, 97] [2]\n",
      "8 2\n",
      "evaluating model:  openai_davinci\n",
      "evaluating model:  pegasus\n",
      "evaluating model:  openai_curie (0 shot)\n",
      "evaluating model:  openai_text-davinci-002\n",
      "evaluating model:  openai_text-curie-001\n",
      "evaluating model:  cohere_xlarge-20220609\n",
      "evaluating model:  openai_text-curie-001 (0 shot)\n",
      "evaluating model:  openai_text-davinci-002 (0 shot)\n",
      "evaluating model:  openai_text-ada-001 (0 shot)\n",
      "evaluating model:  together_opt-175b\n",
      "evaluating model:  openai_davinci (0 shot)\n",
      "evaluating model:  openai_curie\n",
      "evaluating model:  openai_ada (0 shot)\n",
      "evaluating model:  brio\n",
      "evaluating model:  together_glm\n",
      "evaluating model:  openai_ada\n",
      "evaluating model:  anthropic_stanford-online-all-v4-s3\n",
      "evaluating model:  openai_text-ada-001\n"
     ]
    }
   ],
   "source": [
    "#尝试最高分挑一个，最低分挑一个，然后看能不能改善结果\n",
    "\n",
    "def get_template_list(news_list,summary_list,score_list):\n",
    "    \n",
    "    res_news_list = []\n",
    "    res_summary_list = []\n",
    "    res_score_list = []\n",
    "    \n",
    "    max_score = max(score_list)\n",
    "    min_score = min(score_list)\n",
    "    # max_index = score_list.index(max_score)\n",
    "    # min_index = score_list.index(min_score)\n",
    "    \n",
    "    max_indexes = [index for index, value in enumerate(score_list) if value == max_score]\n",
    "    min_indexes = [index for index, value in enumerate(score_list) if value == min_score]\n",
    "    \n",
    "    print(max_indexes, min_indexes)\n",
    "    max_index = max_indexes[0]\n",
    "    min_index = min_indexes[0]\n",
    "    \n",
    "    print(max_index, min_index)\n",
    "    \n",
    "    res_news_list.append(news_list[max_index])\n",
    "    res_summary_list.append(summary_list[max_index])\n",
    "    res_score_list.append(score_list[max_index])\n",
    "    \n",
    "    res_news_list.append(news_list[min_index])\n",
    "    res_summary_list.append(summary_list[min_index])\n",
    "    res_score_list.append(score_list[min_index])\n",
    "    \n",
    "    return res_news_list, res_summary_list, res_score_list\n",
    "\n",
    "\n",
    "cnndm_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json')\n",
    "\n",
    "target_dataset = cnndm_liker\n",
    "template_dataset = cnndm_liker\n",
    "\n",
    "aspect = \"relevance\"\n",
    "\n",
    "tag_aspect = \"relevance\"\n",
    "\n",
    "model_list = list(set(target_dataset['model'].tolist()))\n",
    "model_list.remove('reference')\n",
    "\n",
    "\n",
    "candiate_news = template_dataset[template_dataset[\"model\"]==\"reference\"][\"article\"].to_list()\n",
    "candiate_summary = template_dataset[template_dataset[\"model\"]==\"reference\"][\"summary\"].to_list()\n",
    "candiate_score = template_dataset[template_dataset[\"model\"]==\"reference\"][tag_aspect].to_list()\n",
    "\n",
    "template_news, template_summary, template_score =  get_template_list(candiate_news, candiate_summary, candiate_score)\n",
    "\n",
    "\n",
    "\n",
    "model_eva_dict= {}\n",
    "human_eva_dict = {}\n",
    "\n",
    "for m in model_list:\n",
    "    print(\"evaluating model: \", m)\n",
    "    tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "    \n",
    "    tmp_news_list = tmp_dataset['article'].tolist()\n",
    "    \n",
    "    tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "    tmp_score_list = tmp_dataset[tag_aspect].tolist()\n",
    "    score_list = loop_score_api_chat(tmp_news_list, tmp_summary_list, client, llm_model, aspect, \n",
    "                                     refer_news = template_news, refer_summary = template_summary, refer_score = template_score)\n",
    "    \n",
    "    model_eva_dict[m] = score_list\n",
    "    human_eva_dict[m] = tmp_score_list\n",
    "    \n",
    "\n",
    "    \n",
    "#save the result\n",
    "with open('./evaluate_result/'+'qwen72_cnndm_'+tag_aspect+'_template_cnndm_max8_min2_eva.json', 'w') as fp:\n",
    "    json.dump(model_eva_dict, fp)\n",
    "    \n",
    "# with open('./evaluate_result/'+'summeval_human_eva_dict.json', 'w') as fp:\n",
    "#     json.dump(human_eva_dict, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation of system level is  0.6911252597850467\n",
      "spearmans correlation of system level is  0.8312669486241189\n",
      "kendalltau correlation of summary level is  0.2414760347942291\n",
      "spearmans correlation of summary level is  0.2754772233976758\n"
     ]
    }
   ],
   "source": [
    "correlation_score(model_eva_dict, human_eva_dict)\n",
    "\n",
    "#cnndm\n",
    "#none\n",
    "#index None\n",
    "# correlation of system level is  0.724262721160196\n",
    "# correlation of summary level is  nan   \n",
    "\n",
    "\n",
    "#max 8 min 2\n",
    "# kendalltau correlation of system level is  0.6911252597850467\n",
    "# spearmans correlation of system level is  0.8312669486241189\n",
    "# kendalltau correlation of summary level is  0.2414760347942291\n",
    "# spearmans correlation of summary level is  0.2754772233976758\n",
    "\n",
    "#max 21 min 2\n",
    "# kendalltau correlation of system level is  0.5940885257860047\n",
    "# spearmans correlation of system level is  0.7390189760464155\n",
    "# kendalltau correlation of summary level is  0.22436172250939312\n",
    "# spearmans correlation of summary level is  0.2557292427393978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating without example first\n",
      "[2]\n",
      "2\n",
      "evaluating model:  together_glm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb 单元格 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m tmp_summary_list \u001b[39m=\u001b[39m tmp_dataset[\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m tmp_score_list \u001b[39m=\u001b[39m tmp_dataset[aspect]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m score_list \u001b[39m=\u001b[39m loop_score_api_chat(tmp_news_list, tmp_summary_list, client, llm_model, aspect, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m                                  refer_news \u001b[39m=\u001b[39;49m template_news, refer_summary \u001b[39m=\u001b[39;49m template_summary, refer_score \u001b[39m=\u001b[39;49m template_score)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m model_eva_dict[m] \u001b[39m=\u001b[39m score_list\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m human_eva_dict[m] \u001b[39m=\u001b[39m tmp_score_list\n",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb 单元格 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m         summary \u001b[39m=\u001b[39m summary_list[i]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m         user_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNews: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mnews\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mGenerated summary: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39msummary\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m         score_list\u001b[39m.\u001b[39mappend(score_api_chat(client, model_name, prompt, user_input))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score_list\n",
      "\u001b[1;32m/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb 单元格 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore_api_chat\u001b[39m(client, model_name, prompt, user_input, temperature \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     chat_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m             model\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m             messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m                 {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                 {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: user_input},\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m                 \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m             ],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m             temperature\u001b[39m=\u001b[39;49mtemperature\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39m# \"\\n\\nReference summary: \"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m         \u001b[39m#         +reference_summary+\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/summary_benchmark/test/LLM_evaluate.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     res \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    664\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    665\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    666\u001b[0m             {\n\u001b[1;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    669\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    670\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    671\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    672\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    673\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    674\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    675\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    676\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    677\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    678\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    679\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    680\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    681\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    682\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    683\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    684\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    685\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    686\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    687\u001b[0m             },\n\u001b[1;32m    688\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    689\u001b[0m         ),\n\u001b[1;32m    690\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    691\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    692\u001b[0m         ),\n\u001b[1;32m    693\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    694\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    695\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    696\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    890\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    891\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    892\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    893\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    894\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    895\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/openai/_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mauth\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_auth\n\u001b[1;32m    917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    919\u001b[0m         request,\n\u001b[1;32m    920\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    921\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    922\u001b[0m     )\n\u001b[1;32m    923\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    924\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered httpx.TimeoutException\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[39m=\u001b[39m pool_request\u001b[39m.\u001b[39mwait_for_connection(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[39m.\u001b[39;49mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[39m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[39m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#want to evaluate without example first\n",
    "#then find which is the best and worst, then use them as template\n",
    "\n",
    "def get_template_list(candiate_news, candiate_summary, candiate_score,client, llm_model, aspect):\n",
    "    res_news_list = []\n",
    "    res_summary_list = []\n",
    "    res_score_list = []\n",
    "    \n",
    "    print(\"evaluating without example first\")\n",
    "    #first evaluate\n",
    "    score_list = loop_score_api_chat(candiate_news, candiate_summary, client, llm_model, aspect, \n",
    "                                     refer_news = None, refer_summary = None, refer_score = None)\n",
    "    \n",
    "    for i in range(len(score_list)):\n",
    "        score_list[i] = abs(score_list[i]-candiate_score[i])\n",
    "    \n",
    "    \n",
    "    max_diff_score = max(score_list)\n",
    "    \n",
    "    max_indexes = [index for index, value in enumerate(score_list) if value == max_diff_score]\n",
    "    \n",
    "    print(max_indexes)\n",
    "    max_index = max_indexes[0]\n",
    "    \n",
    "    print(max_index)\n",
    "    \n",
    "    res_news_list.append(candiate_news[max_index])\n",
    "    res_summary_list.append(candiate_news[max_index])\n",
    "    res_score_list.append(candiate_score[max_index])\n",
    "    \n",
    "    \n",
    "    return res_news_list, res_summary_list, res_score_list, max_index\n",
    "\n",
    "\n",
    "\n",
    "cnndm_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json')\n",
    "\n",
    "target_dataset = cnndm_liker\n",
    "template_dataset = cnndm_liker\n",
    "\n",
    "aspect = \"relevance\"\n",
    "\n",
    "\n",
    "model_list = list(set(target_dataset['model'].tolist()))\n",
    "model_list.remove('reference')\n",
    "\n",
    "\n",
    "candiate_news = template_dataset[template_dataset[\"model\"]==\"reference\"][\"article\"].to_list()\n",
    "candiate_summary = template_dataset[template_dataset[\"model\"]==\"reference\"][\"summary\"].to_list()\n",
    "candiate_score = template_dataset[template_dataset[\"model\"]==\"reference\"][aspect].to_list()\n",
    "\n",
    "template_news, template_summary, template_score, max_index =  get_template_list(candiate_news, candiate_summary, candiate_score, client, llm_model, aspect)\n",
    "\n",
    "\n",
    "\n",
    "model_eva_dict= {}\n",
    "human_eva_dict = {}\n",
    "\n",
    "for m in model_list:\n",
    "    print(\"evaluating model: \", m)\n",
    "    tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "    \n",
    "    tmp_news_list = tmp_dataset['article'].tolist()\n",
    "    \n",
    "    tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "    tmp_score_list = tmp_dataset[aspect].tolist()\n",
    "    score_list = loop_score_api_chat(tmp_news_list, tmp_summary_list, client, llm_model, aspect, \n",
    "                                     refer_news = template_news, refer_summary = template_summary, refer_score = template_score)\n",
    "    \n",
    "    model_eva_dict[m] = score_list\n",
    "    human_eva_dict[m] = tmp_score_list\n",
    "    \n",
    "\n",
    "    \n",
    "#save the result\n",
    "with open('./evaluate_result/'+'qwen72_cnndm_'+aspect+'_template_cnndm_diff_max'+str(max_index)+'_eva.json', 'w') as fp:\n",
    "    json.dump(model_eva_dict, fp)\n",
    "\n",
    "correlation_score(model_eva_dict, human_eva_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
