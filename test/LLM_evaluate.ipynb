{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this noetbook focus on the evaluation of summary quality using LLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "from scipy.stats import kendalltau\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8222/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "llm_model = \"Qwen/Qwen1.5-72B-Chat\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function block\n",
    "\n",
    "#genreate prompt\n",
    "def generate_prompt(few_shot: int, aspect:str, refer_news = None, refer_summary = None, refer_score = None):\n",
    "    prompt = \"You are a helpful summary assistant.\"\n",
    "    if(aspect!=\"Faithfulness\"):\n",
    "        base_prompt = \"In this task, you will be provided with a news article and a generated summary.\\n\"\\\n",
    "                \"Your task is to rate the \" + aspect +\" of the generated summary with a score from 1 to 5, \"\\\n",
    "                \"where 1 is the lowest and 5 is the highest.\\n\"\\\n",
    "                \"Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\"\n",
    "        if(few_shot==0):\n",
    "            prompt = base_prompt+ \"Example Response:\\n\"+aspect+\" (1-5): 3\"\n",
    "        else:\n",
    "            shot_num = len(refer_news)\n",
    "            if shot_num>2:\n",
    "                shot_num = 2\n",
    "            if(shot_num==1):\n",
    "                base_prompt+=\"This is an example:\\n\"\n",
    "            else:\n",
    "                base_prompt+=\"These are examples:\\n\"\n",
    "            for i in range(shot_num):\n",
    "                prompt = base_prompt + \"News: \"+refer_news[i]+\"\\nGenerated summary: \"+refer_summary[i]+\"\\n\"+aspect+\" (1-5): \"+str(refer_score[i])+\"\\n\"\n",
    "    else:\n",
    "        base_prompt = \"In this task, you will be provided with a news article and a generated summary.\\n\"\\\n",
    "                \"Your task is to rate the \" + aspect +\" of the generated summary with a score 0 or 1, \"\\\n",
    "                \"where 1 is not fact and 1 is fact.\\n\"\\\n",
    "                \"Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\\n\"\n",
    "        \n",
    "        if(few_shot==0):\n",
    "            prompt = base_prompt+ \"Example Response:\\n\"+aspect+\" (0,1): 1\"\n",
    "        else:\n",
    "            shot_num = len(refer_news)\n",
    "            if shot_num>2:\n",
    "                shot_num = 2\n",
    "            if(shot_num==1):\n",
    "                base_prompt+=\"This is an example:\\n\"\n",
    "            else:\n",
    "                base_prompt+=\"These are examples:\\n\"\n",
    "            for i in range(shot_num):\n",
    "                prompt = base_prompt + \"News: \"+refer_news[i]+\"\\nGenerated summary: \"+refer_summary[i]+\"\\n\"+aspect+\" (0,1): \"+str(refer_score[i])+\"\\n\"\n",
    "                                   \n",
    "    return prompt\n",
    "    \n",
    "\n",
    "#get socre using openai api\n",
    "def score_api_chat(client, model_name, prompt, user_input, temperature = 0):\n",
    "    chat_response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                \n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        # \"\\n\\nReference summary: \"\n",
    "        #         +reference_summary+\n",
    "    res = chat_response.choices[0].message.content\n",
    "        \n",
    "    match = re.search(r':\\s*(\\d+)', res)   \n",
    "    score = match.group(1) if match is not None else 1\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "def loop_score_api_chat(news_list, summary_list, client, model_name, aspect, \n",
    "                        refer_news = None, refer_summary = None, refer_score = None,temperature = 0):\n",
    "    \n",
    "    score_list = []\n",
    "    prompt = \"\"\n",
    "    if(refer_news is None):\n",
    "        prompt = generate_prompt(0, aspect)  \n",
    "    else:\n",
    "        prompt = generate_prompt(1, aspect, refer_news, refer_summary, refer_score)\n",
    "\n",
    "    \n",
    "    for i in range(len(news_list)):\n",
    "            news = news_list[i]\n",
    "            summary = summary_list[i]\n",
    "            user_input = \"News: \"+news+\"\\n\\Generated summary: \"+summary\n",
    "            score_list.append(score_api_chat(client, model_name, prompt, user_input))\n",
    "    return score_list   \n",
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "    print(\"correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model:  M20\n",
      "evaluating model:  M5\n",
      "evaluating model:  M15\n",
      "evaluating model:  M8\n",
      "evaluating model:  M23\n",
      "evaluating model:  M11\n",
      "evaluating model:  M10\n",
      "evaluating model:  M17\n",
      "evaluating model:  M14\n",
      "evaluating model:  M9\n",
      "evaluating model:  M2\n",
      "evaluating model:  M13\n",
      "evaluating model:  M22\n",
      "evaluating model:  M12\n",
      "evaluating model:  M1\n"
     ]
    }
   ],
   "source": [
    "#load the dataset to processing\n",
    "\n",
    "cnndm_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_cnndm_average.json')\n",
    "\n",
    "xsum_liker = pd.read_json('/home/xbr/LLM/benchmark_llm_summarization/likert_evaluation_results_xsum_average.json')\n",
    "\n",
    "summeval = pd.read_json('./filter_annotations_.jsonl')\n",
    "\n",
    "\n",
    "target_dataset = summeval\n",
    "template_dataset = summeval\n",
    "template_index = \"None\"\n",
    "aspect = \"relevance\"\n",
    "\n",
    "tag_aspect = \"expert_relevance\"\n",
    "\n",
    "\n",
    "model_list = list(set(target_dataset['model'].tolist()))\n",
    "\n",
    "model_list.remove('M0')\n",
    "\n",
    "model_eva_dict= {}\n",
    "human_eva_dict = {}\n",
    "# total_summary_level = 0\n",
    "\n",
    "# ['Relevance', 'Coherence', 'Consistency', 'Faithfulness', 'Informativeness']\n",
    "\n",
    "\n",
    "\n",
    "# template_news = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i]['article'] for i in range(2,4)]\n",
    "# template_summary = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i]['summary'] for i in range(2,4)]\n",
    "# template_score = [template_dataset[(template_dataset['model']==\"reference\" )].to_dict(orient='records')[i][aspect] for i in range(2,4)]\n",
    "\n",
    "template_news = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i]['article'] for i in range(1)]\n",
    "template_summary = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i]['summary'] for i in range(1)]\n",
    "template_score = [template_dataset[(template_dataset['model']==\"M0\" )].to_dict(orient='records')[i][tag_aspect] for i in range(1)]\n",
    "\n",
    "# template_news = None\n",
    "# template_summary = None\n",
    "# template_score = None\n",
    "\n",
    "\n",
    "\n",
    "for m in model_list:\n",
    "    print(\"evaluating model: \", m)\n",
    "    tmp_dataset = target_dataset[(target_dataset['model']==m )]\n",
    "    \n",
    "    tmp_news_list = tmp_dataset['article'].tolist()\n",
    "    \n",
    "    tmp_summary_list = tmp_dataset['summary'].tolist()\n",
    "    tmp_score_list = tmp_dataset[tag_aspect].tolist()\n",
    "    score_list = loop_score_api_chat(tmp_news_list, tmp_summary_list, client, llm_model, aspect, \n",
    "                                     refer_news = template_news, refer_summary = template_summary, refer_score = template_score)\n",
    "    \n",
    "    model_eva_dict[m] = score_list\n",
    "    human_eva_dict[m] = tmp_score_list\n",
    "    \n",
    "\n",
    "    \n",
    "#save the result\n",
    "with open('./evaluate_result/'+'qwen72_summeval_'+tag_aspect+'_template_summeval_'+str(template_index)+'_eva.json', 'w') as fp:\n",
    "    json.dump(model_eva_dict, fp)\n",
    "    \n",
    "with open('./evaluate_result/'+'summeval_human_eva_dict.json', 'w') as fp:\n",
    "    json.dump(human_eva_dict, fp)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation of system level is  0.6315861769395361\n",
      "correlation of summary level is  0.32975744251357864\n"
     ]
    }
   ],
   "source": [
    "correlation_score(model_eva_dict, human_eva_dict)\n",
    "# for i in model_eva_dict.keys():\n",
    "#     print(i, \":\", np.mean(model_eva_dict[i]), np.mean(human_eva_dict[i]))        \n",
    "\n",
    "#cnndm\n",
    "#index 0\n",
    "# correlation of system level is  0.813119124543455\n",
    "# correlation of summary level is  0.1472597601736428\n",
    "\n",
    "#idnex 1\n",
    "# correlation of system level is  0.7777777777777779\n",
    "# correlation of summary level is  0.20898023319686118\n",
    "\n",
    "#index 2\n",
    "# correlation of system level is  0.33333333333333337\n",
    "# correlation of summary level is  0.08772663469686875\n",
    "\n",
    "#index 4\n",
    "# correlation of system level is  0.6954252406248034\n",
    "# correlation of summary level is  nan\n",
    "\n",
    "#index None\n",
    "# correlation of system level is  0.724262721160196\n",
    "# correlation of summary level is  nan   \n",
    "\n",
    "#index 0&1\n",
    "# correlation of system level is  0.6997042637035166\n",
    "# correlation of summary level is  0.24644284212607812\n",
    "\n",
    "\n",
    "\n",
    "#xsum\n",
    "#None\n",
    "# correlation of system level is  0.7852067275033966\n",
    "# correlation of summary level is  0.24612884627714107\n",
    "\n",
    "#index 1\n",
    "# correlation of system level is  0.7158720324782436\n",
    "# correlation of summary level is  0.2570950849356606\n",
    "\n",
    "#index 2\n",
    "# correlation of system level is  0.2370435403783839\n",
    "# correlation of summary level is  0.017176762945798965\n",
    "\n",
    "#index 1 2\n",
    "# correlation of system level is  0.7306322805705787\n",
    "# correlation of summary level is  0.28464519005709527\n",
    "\n",
    "#index 3\n",
    "# correlation of system level is  0.7896732729399182\n",
    "# correlation of summary level is  0.2844474928173168\n",
    "\n",
    "# #index 2 3\n",
    "# correlation of system level is  0.8044335210322533\n",
    "# correlation of summary level is  0.2894187475986224\n",
    "\n",
    "\n",
    "\n",
    "#summeval\n",
    "#None\n",
    "# correlation of system level is  0.8140776760988245\n",
    "# correlation of summary level is  nan\n",
    "#index 1\n",
    "# correlation of system level is  0.6315861769395361\n",
    "# correlation of summary level is  0.32975744251357864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation of system level is  0.21123147583502386\n",
      "correlation of summary level is  0.16372567961293896\n"
     ]
    }
   ],
   "source": [
    "# dict1 = json.load(open('./evaluate_result/qwen72_cnndm_relevance_template_cnndm_2_eva.json'))\n",
    "\n",
    "# dict2 = json.load(open('./evaluate_result/qwen72_cnndm_relevance_template_cnndm_0_1_eva.json'))\n",
    "\n",
    "# correlation_score(dict1, dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#尝试最高分挑一个，最低分挑一个，然后看能不能改善结果\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
