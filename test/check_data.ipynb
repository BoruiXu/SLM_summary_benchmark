{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau,pearsonr,spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('../../SummEval/model_annotations.aligned.paired.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = data.to_dict(orient='records')\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     tmp = dic[i]['expert_annotations']\n",
    "    \n",
    "#     for j in tmp:\n",
    "#         print(j['relevance'])\n",
    "#     print(\"==================\")\\\n",
    "# print(data.columns)\n",
    "\n",
    "# print(data.iloc[0]['references'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# print(data.columns)\n",
    "\n",
    "# print(data.iloc[1][\"text\"])\n",
    "\n",
    "# print(data.iloc[1][\"references\"])\n",
    "\n",
    "#get new dataset\n",
    "#'coherence': 2, 'consistency': 1, 'fluency': 4, 'relevance': 2\n",
    "\n",
    "def get_score(aspect, input_list):\n",
    "    tmp = 0\n",
    "    for i in input_list:\n",
    "        tmp += i[aspect]\n",
    "    return tmp/len(input_list)\n",
    "\n",
    "def get_score_list(aspect, input_list):\n",
    "    tmp = []\n",
    "    for i in input_list:\n",
    "        tmp.append(i[aspect])\n",
    "    return tmp\n",
    "\n",
    "\n",
    "# dic = data.to_dict(orient='records')\n",
    "\n",
    "# model = (data[\"model_id\"].unique()).tolist()\n",
    "\n",
    "\n",
    "result = []\n",
    "\n",
    "# result_list_a = []\n",
    "# result_list_b = []\n",
    "\n",
    "\n",
    "# for m in model:\n",
    "#     dic = data[data[\"model_id\"] == m].to_dict(orient='records')\n",
    "    \n",
    "#     a = []\n",
    "#     b = []\n",
    "#     c = []\n",
    "#     d = []\n",
    "#     for i in range(len(dic)):\n",
    "        \n",
    "#         tmp_list = get_score_list(\"relevance\",dic[i][\"expert_annotations\"])\n",
    "#         a.append(float(tmp_list[0]))\n",
    "#         b.append(float(tmp_list[1]))\n",
    "#         c.append(float(tmp_list[2]))\n",
    "#         d.append(float((tmp_list[0]+tmp_list[1]+tmp_list[2]))/3)\n",
    "    \n",
    "#     result_list_a.append(float(np.mean(a)))\n",
    "#     result_list_b.append(float(np.mean(b)))\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "# print(\"kendalltau correlation a b \",kendalltau(a, b))\n",
    "# print(\"kendalltau correlation a c \",kendalltau(a, c))\n",
    "# print(\"kendalltau correlation b c \",kendalltau(b, c))\n",
    "# print(\"system level kendalltau correlation a b \",kendalltau(result_list_a, result_list_b))\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "# #pearsonr\n",
    "# print(\"pearsonr correlation a b \",pearsonr(a, b))\n",
    "# print(\"pearsonr correlation a c \",pearsonr(a, c))\n",
    "# print(\"pearsonr correlation b c \",pearsonr(b, c))\n",
    "# print(\"pearsonr correlation a d \",pearsonr(result_list_a, result_list_d))\n",
    "\n",
    "# print(\"spearmanr correlation a d \",spearmanr(result_list_a, result_list_d))\n",
    "\n",
    "# print(a)\n",
    "# print(d)\n",
    "\n",
    "#save filter data\n",
    "\n",
    "for i in range(len(dic)):\n",
    "    tmp_dic = {}\n",
    "    tmp_dic[\"article\"] = dic[i][\"text\"]\n",
    "    tmp_dic[\"summary\"] = dic[i][\"decoded\"]\n",
    "    tmp_dic[\"model\"] = dic[i][\"model_id\"]\n",
    "    \n",
    "    tmp_dic[\"expert_coherence\"] = get_score(\"coherence\",dic[i][\"expert_annotations\"])\n",
    "    tmp_dic[\"expert_relevance\"] = get_score(\"relevance\",dic[i][\"expert_annotations\"])\n",
    "    tmp_dic[\"expert_consistency\"] = get_score(\"consistency\",dic[i][\"expert_annotations\"])\n",
    "    tmp_dic[\"expert_fluency\"] = get_score(\"fluency\",dic[i][\"expert_annotations\"])\n",
    "    \n",
    "    tmp_dic[\"turker_coherence\"] = get_score(\"coherence\",dic[i][\"turker_annotations\"])\n",
    "    tmp_dic[\"turker_relevance\"] = get_score(\"relevance\",dic[i][\"turker_annotations\"])\n",
    "    tmp_dic[\"turker_consistency\"] = get_score(\"consistency\",dic[i][\"turker_annotations\"])\n",
    "    tmp_dic[\"turker_fluency\"] = get_score(\"fluency\",dic[i][\"turker_annotations\"])\n",
    "    \n",
    "    tmp_dic[\"all_coherence\"] = (tmp_dic[\"expert_coherence\"] + tmp_dic[\"turker_coherence\"])/2\n",
    "    tmp_dic[\"all_relevance\"] = (tmp_dic[\"expert_relevance\"] + tmp_dic[\"turker_relevance\"])/2\n",
    "    tmp_dic[\"all_consistency\"] = (tmp_dic[\"expert_consistency\"] + tmp_dic[\"turker_consistency\"])/2\n",
    "    tmp_dic[\"all_fluency\"] = (tmp_dic[\"expert_fluency\"] + tmp_dic[\"turker_fluency\"])/2\n",
    "    \n",
    "    tmp_dic[\"references\"] = dic[i][\"references\"]\n",
    "    \n",
    "    result.append(tmp_dic)\n",
    "\n",
    "# save to json\n",
    "# with open('filter_annotations_summeval_reference.jsonl', 'w') as outfile:\n",
    "#     json.dump(result, outfile, indent=4)\n",
    "# print(\"done\")    \n",
    "# print(kendalltau([1,2,3,4,5],[2,3,4,5,4])) \n",
    "# print(pearsonr([1,2,3,4,5],[10,20,30,40,50]))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation of system level is  0.8007152437727271\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "    print(\"correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    # total_corr = 0\n",
    "    \n",
    "    # for i in dict1.keys():\n",
    "    #     total_corr+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    # print(\"correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "\n",
    "\n",
    "dict1 = json.load(open('./evaluate_result/summeval_human_eva_dict.json'))\n",
    "\n",
    "dict2 = json.load(open('./evaluate_result/qwen72_summeval_expert_relevance_template_summeval_1_eva.json'))\n",
    "\n",
    "correlation_score(dict1, dict2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'article', 'summary'], dtype='object')\n",
      "Neil Lennon is succeeding Alan Stubbs at Hibernian. The former Celtic manager is looking to add to his list of achievements, as well as find stability amidst an ever-changing game. The move was said to be mutually beneficial, as players have a tendency to perform well under his guidance.\n"
     ]
    }
   ],
   "source": [
    "#check data in benchmark LLM human writer\n",
    "\n",
    "data = pd.read_json('../../benchmark_llm_summarization/filtered_df.json')\n",
    "print(data.columns)\n",
    "\n",
    "data['summary'] = data['summary'].str.strip()\n",
    "\n",
    "print(data.iloc[0][\"summary\"])\n",
    "\n",
    "data = data.to_dict(orient='records')\n",
    "#save\n",
    "import json\n",
    "json_file = 'filtered_df_strip.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Dec 02 2020\n",
      "1        Dec 02 2020\n",
      "2        Dec 02 2020\n",
      "3        Dec 02 2020\n",
      "4        Dec 02 2020\n",
      "            ...     \n",
      "26752    Feb 13 2019\n",
      "26753    Feb 13 2019\n",
      "26754    Feb 13 2019\n",
      "26755    Feb 13 2019\n",
      "26756    Feb 13 2019\n",
      "Name: 1, Length: 26757, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#查看路透特新闻数据集\n",
    "\n",
    "news = pd.read_csv('/home/xbr/LLM/dataset/Reuters_news.csv',header=None)\n",
    "\n",
    "news[1] = news[1].str.replace(\"\\n\",\"\") \n",
    "print(news[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for chiseng-cheang/TempoSum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/chiseng-cheang/TempoSum\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "#try TempoSum dataset\n",
    "import datasets\n",
    "dataset = datasets.load_dataset('chiseng-cheang/TempoSum', 'BBC_future')\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation of system level is  0.7044161301655284\n",
      "spearmans correlation of system level is  0.8660142784299882\n",
      "kendalltau correlation of summary level is  nan\n",
      "spearmans correlation of summary level is  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1789888/2066288017.py:21: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kendalltau, spearmanr\n",
    "import json\n",
    "import numpy as np\n",
    "def correlation_score(dict1, dict2):\n",
    "    #system level\n",
    "    tmp_list1 = []\n",
    "    tmp_list2 = []\n",
    "    for i in dict1.keys():\n",
    "        tmp_list1.append(np.mean(dict1[i]))\n",
    "        tmp_list2.append(np.mean(dict2[i]))\n",
    "        \n",
    "    print(\"kendalltau correlation of system level is \", kendalltau(tmp_list1, tmp_list2)[0])\n",
    "    print(\"spearmans correlation of system level is \", spearmanr(tmp_list1, tmp_list2)[0])\n",
    "    \n",
    "    #summary level\n",
    "    total_corr = 0\n",
    "    total_corr2 = 0\n",
    "    \n",
    "    for i in dict1.keys():\n",
    "        total_corr+=kendalltau(dict1[i], dict2[i])[0]\n",
    "        total_corr2+=spearmanr(dict1[i], dict2[i])[0]\n",
    "    print(\"kendalltau correlation of summary level is \", total_corr/len(dict1.keys()))\n",
    "    print(\"spearmans correlation of summary level is \", total_corr2/len(dict1.keys()))\n",
    "    \n",
    "d1 = json.load(open('./LLM_evaluation_correlation_with_human/qwen/Qwen1.5-72B-Chat_likert_evaluation_results_cnndm_average_relevance_eva.json'))\n",
    "d2 = json.load(open('./LLM_evaluation_correlation_with_human/qwen/human_score_likert_evaluation_results_cnndm_average_relevance_eva.json'))\n",
    "\n",
    "correlation_score(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 19:34:27.435765: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 19:34:27.482493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 19:34:28.298337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [1.0, 1.0], 'recall': [1.0, 1.0], 'f1': [1.0, 1.0], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.38.1)'}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"microsoft/deberta-xlarge-mnli\",lang=\"en\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"microsoft/deberta-xlarge-mnli\",lang=\"en\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"microsoft/deberta-xlarge-mnli\",lang=\"en\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"microsoft/deberta-xlarge-mnli\",lang=\"en\")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "factkb_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", padding=\"max_length\", truncation=True)\n",
    "factkb_model = AutoModelForSequenceClassification.from_pretrained(\"bunsenfeng/FactKB\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否只包含一个单数字: 5 asdasdas False\n",
      "是否只包含一个单数字: 10 False\n",
      "是否只包含一个单数字: a False\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# t1 = \"1112123123\"\n",
    "# t2 = \"213214234234\"\n",
    "# input_factkb = [[t1, t2]]\n",
    "# factkb_tokens = factkb_tokenizer(input_factkb, return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(factkb_model.device)\n",
    "# factkb_logits = factkb_model(**factkb_tokens).logits\n",
    "# factkb_res = torch.softmax(factkb_logits, dim=1)\n",
    "# print(factkb_res)\n",
    "\n",
    "import re\n",
    "\n",
    "def is_single_digit_string(text):\n",
    "    # 使用正则表达式匹配字符串是否只包含一个单数字\n",
    "    match = re.match(r'^\\d$', text)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 测试字符串\n",
    "text1 = \"5 asdasdas\"\n",
    "text2 = \"10\"\n",
    "text3 = \"a\"\n",
    "\n",
    "# 检查字符串是否只包含一个单数字\n",
    "print(\"是否只包含一个单数字:\", text1, is_single_digit_string(text1))\n",
    "print(\"是否只包含一个单数字:\", text2, is_single_digit_string(text2))\n",
    "print(\"是否只包含一个单数字:\", text3, is_single_digit_string(text3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
