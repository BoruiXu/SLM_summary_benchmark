{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 22:09:04.684655: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-01 22:09:04.729425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-01 22:09:05.523438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "bert_score = load(\"bertscore\")\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[preds], references=[refs], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.8450139760971069], 'recall': [0.5974714756011963], 'f1': [0.7000024914741516], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.39.3)'}\n"
     ]
    }
   ],
   "source": [
    "a = 'Manchester City defeated Manchester United 3-1 with a double from Phil Foden and a goal from Erling Haaland.'\n",
    "b = 'Manchester City came from behind to beat Manchester United 3-1 in a thrilling Premier League match. Phil Foden scored a brace, including a stunning equalizer, and Erling Haaland added a third goal for City in stoppage time. Marcus Rashford had given United an early lead with a sensational strike, but City dominated the game and deservedly took all three points. The win moves City to within a point of leaders Liverpool, while United remain in sixth place.'\n",
    "print(BertScore(b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n",
      "0.7546372020244598\n",
      "newsroom\n",
      "0.7415115525722503\n",
      "cnndm\n",
      "0.759126748085022\n",
      "bbc2024\n",
      "0.7524805088043213\n"
     ]
    }
   ],
   "source": [
    "qwen1_5_files = ['xsum_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'newsroom_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'cnndm_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'bbc2024_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama2_files = ['xsum_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'newsroom_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'cnndm_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'bbc2024_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama3_files = ['xsum_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl']\n",
    "\n",
    "base_qwen_path = './qwen1_5/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama2_path = './llama2/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama3_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama3_70b_summary/'\n",
    "\n",
    "for i in range(len(qwen1_5_files)):\n",
    "    name = qwen1_5_files[i].split('_')[0]\n",
    "    print(name)\n",
    "    \n",
    "    f1 = base_qwen_path + '/' + qwen1_5_files[i]\n",
    "    f2 = base_llama2_path + '/' + llama2_files[i]\n",
    "    f3 = base_llama3_path + '/' + llama3_files[i]\n",
    "    #use json to load f1 and f2\n",
    "    qwen = json.load(open(f1))\n",
    "    llama2 = json.load(open(f2))\n",
    "    \n",
    "    llama3 = json.load(open(f3))\n",
    "    \n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for j in range(len(qwen)):\n",
    "        score+= BertScore(llama2[j]['target'], llama3[j][\"qwen_reference_summary\"])['f1'][0]\n",
    "        \n",
    "    print(score/len(qwen))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be1445cccbf4c9ebfff697b930c4392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 399/500 [09:33<02:25,  1.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb 单元格 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m news \u001b[39m=\u001b[39m llama2[j][\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m reference \u001b[39m=\u001b[39m llama2[j][\u001b[39m'\u001b[39m\u001b[39mqwen_reference_summary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m response, history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mchat(tokenizer, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNews: \u001b[39;49m\u001b[39m{\u001b[39;49;00mnews\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mSummarize the news in two sentences. Summary:\u001b[39;49m\u001b[39m'\u001b[39;49m, history\u001b[39m=\u001b[39;49m[],do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m response \u001b[39m=\u001b[39m clean_summary(response)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#f1 bertscore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:1042\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.chat\u001b[0;34m(self, tokenizer, query, history, role, max_length, num_beams, do_sample, top_p, temperature, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1040\u001b[0m eos_token_id \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39meos_token_id, tokenizer\u001b[39m.\u001b[39mget_command(\u001b[39m\"\u001b[39m\u001b[39m<|user|>\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1041\u001b[0m                 tokenizer\u001b[39m.\u001b[39mget_command(\u001b[39m\"\u001b[39m\u001b[39m<|observation|>\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m-> 1042\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgen_kwargs, eos_token_id\u001b[39m=\u001b[39;49meos_token_id)\n\u001b[1;32m   1043\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m][\u001b[39mlen\u001b[39m(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]):\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1044\u001b[0m response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1545\u001b[0m         input_ids,\n\u001b[1;32m   1546\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1547\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1548\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1549\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1550\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1551\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1552\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1553\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1554\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1555\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2405\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2406\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2407\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2408\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2409\u001b[0m )\n\u001b[1;32m   2411\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:941\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, return_last_logit)\u001b[0m\n\u001b[1;32m    938\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[1;32m    939\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 941\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    942\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    943\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    944\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    945\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    946\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    947\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    948\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    949\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    950\u001b[0m )\n\u001b[1;32m    952\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    953\u001b[0m \u001b[39mif\u001b[39;00m return_last_logit:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:834\u001b[0m, in \u001b[0;36mChatGLMModel.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, full_attention_mask, past_key_values, inputs_embeds, use_cache, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m rotary_pos_emb \u001b[39m=\u001b[39m rotary_pos_emb\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    833\u001b[0m \u001b[39m# Run encoder.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m hidden_states, presents, all_hidden_states, all_self_attentions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    835\u001b[0m     inputs_embeds, full_attention_mask, rotary_pos_emb\u001b[39m=\u001b[39;49mrotary_pos_emb,\n\u001b[1;32m    836\u001b[0m     kv_caches\u001b[39m=\u001b[39;49mpast_key_values, use_cache\u001b[39m=\u001b[39;49muse_cache, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    840\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [hidden_states, presents, all_hidden_states, all_self_attentions] \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:641\u001b[0m, in \u001b[0;36mGLMTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches, use_cache, output_hidden_states)\u001b[0m\n\u001b[1;32m    631\u001b[0m     layer_ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    632\u001b[0m         layer,\n\u001b[1;32m    633\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m         use_reentrant\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     layer_ret \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    642\u001b[0m         hidden_states,\n\u001b[1;32m    643\u001b[0m         attention_mask,\n\u001b[1;32m    644\u001b[0m         rotary_pos_emb,\n\u001b[1;32m    645\u001b[0m         kv_cache\u001b[39m=\u001b[39;49mkv_caches[index],\n\u001b[1;32m    646\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m hidden_states, kv_cache \u001b[39m=\u001b[39m layer_ret\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:544\u001b[0m, in \u001b[0;36mGLMBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    542\u001b[0m layernorm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    543\u001b[0m \u001b[39m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m attention_output, kv_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    545\u001b[0m     layernorm_output,\n\u001b[1;32m    546\u001b[0m     attention_mask,\n\u001b[1;32m    547\u001b[0m     rotary_pos_emb,\n\u001b[1;32m    548\u001b[0m     kv_cache\u001b[39m=\u001b[39;49mkv_cache,\n\u001b[1;32m    549\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[39m# Residual connection.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_residual_connection_post_layernorm:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:408\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m# apply relative positional encoding (rotary embedding)\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m rotary_pos_emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     query_layer \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n\u001b[1;32m    409\u001b[0m     key_layer \u001b[39m=\u001b[39m apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n\u001b[1;32m    411\u001b[0m \u001b[39m# adjust key and value for inference\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate ChatGLM3\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "def clean_summary(completion):\n",
    "        completion = completion[2:] if completion.startswith(\"\\n\\n\") else completion\n",
    "        completion = completion.replace('The news summary is: \\\"', '')\n",
    "        completion_list = completion.split('\\n')\n",
    "        if(len(completion_list)<3):\n",
    "            completion = completion_list[-1]\n",
    "        else:\n",
    "            completion = completion.split('\\n')[2]\n",
    "        candidate = completion\n",
    "        complete_sentences = re.findall(r'[^.!?]*[.!?]', completion)\n",
    "        # 将匹配到的完整句子连接成一个新的字符串\n",
    "        completion = ''.join(complete_sentences)\n",
    "        if(completion==''):\n",
    "            completion = candidate\n",
    "        return completion\n",
    "    \n",
    "    \n",
    "qwen1_5_files = ['xsum_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl']\n",
    "base_qwen1_5_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen72b_summary'\n",
    "\n",
    "\n",
    "llama2_files = ['xsum_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl']\n",
    "base_llama2_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama2_70b_summary'\n",
    " \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "average_result = dict()\n",
    "for i in range(len(llama2_files)):\n",
    "    name = llama2_files[i].split('_')[0]\n",
    "    print(name)\n",
    "    f1 = base_llama2_path + '/' + llama2_files[i]\n",
    "    #use json to load f1 \n",
    "    llama2 = json.load(open(f1))\n",
    "    \n",
    "    \n",
    "    result_list = list()\n",
    "    result_name = llama2_files[i] + '_chatglm3.jsonl'\n",
    "\n",
    "    average_score = 0\n",
    "    for j in tqdm(range(len(llama2))):\n",
    "        news = llama2[j]['article']\n",
    "        reference = llama2[j]['qwen_reference_summary']\n",
    "        \n",
    "        response, history = model.chat(tokenizer, f'News: {news}\\nSummarize the news in two sentences. Summary:', history=[],do_sample=False)\n",
    "        response = clean_summary(response)\n",
    "        \n",
    "        #f1 bertscore\n",
    "        score = BertScore(reference, response)['f1'][0]\n",
    "        average_score += score\n",
    "        \n",
    "        tmp_dict = dict()\n",
    "        tmp_dict['article'] = news\n",
    "        tmp_dict['target'] = reference\n",
    "        tmp_dict['filtered_resps'] = response\n",
    "        tmp_dict['bertscore_f1'] = score\n",
    "        \n",
    "        result_list.append(tmp_dict)\n",
    "         \n",
    "        \n",
    "    average_result[name] = average_score/len(llama2)\n",
    "    \n",
    "    #save result to jsonl\n",
    "    with open('./llama2/chatglm3/'+result_name, 'w') as f:\n",
    "        json.dump(result_list, f, indent=4)\n",
    "    with open('./llama2/chatglm3/average.jsonl', 'w') as f:\n",
    "        json.dump([average_result], f, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725672722876072\n"
     ]
    }
   ],
   "source": [
    "file = './qwen1_5/chatglm3/bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl'\n",
    "\n",
    "data = json.load(open(file))\n",
    "\n",
    "score = 0\n",
    "for i in data:\n",
    "    score += i['bertscore_f1']\n",
    "print(score/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min score 65.72307348251343\n",
      "max score 91.55803322792053\n",
      "77.38927828073501\n",
      "newsroom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb 单元格 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m all_len \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(qwen)):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39m# print(llama2[j])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39m# score+= BertScore(chatglm3[j][\"filtered_resps\"], phi3[j][\"filtered_resps\"][0])['f1'][0]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     \u001b[39m# score+= BertScore(qwen2_7b[j][\"filtered_resps\"][0], phi3[j][\"filtered_resps\"][0])['f1'][0]\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     tmp_score \u001b[39m=\u001b[39m BertScore(llama3[j][\u001b[39m\"\u001b[39;49m\u001b[39mqwen_reference_summary\u001b[39;49m\u001b[39m\"\u001b[39;49m],qwen[j][\u001b[39m\"\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m\"\u001b[39;49m])[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39m# score+= BertScore(phi3[j][\"filtered_resps\"][0], qwen2[j][\"qwen_reference_summary\"])['f1'][0]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39m# tmp_score = BertScore(qwen2_7b[j][\"filtered_resps\"][0], llama2[j]['target'])['f1'][0]*100\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     \u001b[39m# tmp_len = len(phi2[j][\"filtered_resps\"][0].split(' '))\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m     list_score\u001b[39m.\u001b[39mappend(tmp_score)\n",
      "\u001b[1;32m/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb 单元格 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mBertScore\u001b[39m(refs, preds):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     bert_score_res \u001b[39m=\u001b[39m bert_score\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49m[preds], references\u001b[39m=\u001b[39;49m[refs], model_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/deberta-xlarge-mnli\u001b[39;49m\u001b[39m\"\u001b[39;49m, lang\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bert_score_res\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/evaluate/module.py:462\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m inputs \u001b[39m=\u001b[39m {input_name: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[input_name] \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    461\u001b[0m \u001b[39mwith\u001b[39;00m temp_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed):\n\u001b[0;32m--> 462\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcompute_kwargs)\n\u001b[1;32m    464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b/bertscore.py:203\u001b[0m, in \u001b[0;36mBERTScore._compute\u001b[0;34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcached_bertscorer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer\u001b[39m.\u001b[39mhash \u001b[39m!=\u001b[39m hashcode:\n\u001b[1;32m    189\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached_bertscorer \u001b[39m=\u001b[39m scorer(\n\u001b[1;32m    190\u001b[0m             model_type\u001b[39m=\u001b[39mmodel_type,\n\u001b[1;32m    191\u001b[0m             num_layers\u001b[39m=\u001b[39mnum_layers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m             baseline_path\u001b[39m=\u001b[39mbaseline_path,\n\u001b[1;32m    201\u001b[0m         )\n\u001b[0;32m--> 203\u001b[0m (P, R, F) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcached_bertscorer\u001b[39m.\u001b[39;49mscore(\n\u001b[1;32m    204\u001b[0m     cands\u001b[39m=\u001b[39;49mpredictions,\n\u001b[1;32m    205\u001b[0m     refs\u001b[39m=\u001b[39;49mreferences,\n\u001b[1;32m    206\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    207\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    209\u001b[0m output_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m    210\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: P\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    211\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: R\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m: F\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhashcode\u001b[39m\u001b[39m\"\u001b[39m: hashcode,\n\u001b[1;32m    214\u001b[0m }\n\u001b[1;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m output_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/bert_score/scorer.py:220\u001b[0m, in \u001b[0;36mBERTScorer.score\u001b[0;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[1;32m    217\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39msep_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    218\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mcls_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 220\u001b[0m all_preds \u001b[39m=\u001b[39m bert_cos_score_idf(\n\u001b[1;32m    221\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model,\n\u001b[1;32m    222\u001b[0m     refs,\n\u001b[1;32m    223\u001b[0m     cands,\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer,\n\u001b[1;32m    225\u001b[0m     idf_dict,\n\u001b[1;32m    226\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    227\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    228\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    229\u001b[0m     all_layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_layers,\n\u001b[1;32m    230\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m ref_group_boundaries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     max_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/bert_score/utils.py:616\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39mfor\u001b[39;00m batch_start \u001b[39min\u001b[39;00m iter_range:\n\u001b[1;32m    615\u001b[0m     sen_batch \u001b[39m=\u001b[39m sentences[batch_start : batch_start \u001b[39m+\u001b[39m batch_size]\n\u001b[0;32m--> 616\u001b[0m     embs, masks, padded_idf \u001b[39m=\u001b[39m get_bert_embedding(\n\u001b[1;32m    617\u001b[0m         sen_batch, model, tokenizer, idf_dict, device\u001b[39m=\u001b[39;49mdevice, all_layers\u001b[39m=\u001b[39;49mall_layers\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m     embs \u001b[39m=\u001b[39m embs\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    620\u001b[0m     masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/bert_score/utils.py:455\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    454\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(all_sens), batch_size):\n\u001b[0;32m--> 455\u001b[0m         batch_embedding \u001b[39m=\u001b[39m bert_encode(\n\u001b[1;32m    456\u001b[0m             model,\n\u001b[1;32m    457\u001b[0m             padded_sens[i : i \u001b[39m+\u001b[39;49m batch_size],\n\u001b[1;32m    458\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mmask[i : i \u001b[39m+\u001b[39;49m batch_size],\n\u001b[1;32m    459\u001b[0m             all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(batch_embedding)\n\u001b[1;32m    462\u001b[0m         \u001b[39mdel\u001b[39;00m batch_embedding\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/bert_score/utils.py:351\u001b[0m, in \u001b[0;36mbert_encode\u001b[0;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[1;32m    349\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    350\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 351\u001b[0m     out \u001b[39m=\u001b[39m model(x, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_hidden_states\u001b[39m=\u001b[39;49mall_layers)\n\u001b[1;32m    352\u001b[0m \u001b[39mif\u001b[39;00m all_layers:\n\u001b[1;32m    353\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:974\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    964\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    966\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    967\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    968\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    971\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    972\u001b[0m )\n\u001b[0;32m--> 974\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    975\u001b[0m     embedding_output,\n\u001b[1;32m    976\u001b[0m     attention_mask,\n\u001b[1;32m    977\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    978\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    979\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    981\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:470\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    460\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    461\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    462\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         output_attentions,\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    471\u001b[0m         next_kv,\n\u001b[1;32m    472\u001b[0m         attention_mask,\n\u001b[1;32m    473\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    474\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    475\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    476\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    477\u001b[0m     )\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    480\u001b[0m     hidden_states, att_m \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:383\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    375\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    376\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    382\u001b[0m ):\n\u001b[0;32m--> 383\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    384\u001b[0m         hidden_states,\n\u001b[1;32m    385\u001b[0m         attention_mask,\n\u001b[1;32m    386\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    387\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    388\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    389\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    392\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:316\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    309\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m ):\n\u001b[0;32m--> 316\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    317\u001b[0m         hidden_states,\n\u001b[1;32m    318\u001b[0m         attention_mask,\n\u001b[1;32m    319\u001b[0m         output_attentions,\n\u001b[1;32m    320\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    321\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    322\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    325\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/deberta/modeling_deberta.py:670\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtalking_head:\n\u001b[1;32m    668\u001b[0m     attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_logits_proj(attention_scores\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 670\u001b[0m attention_probs \u001b[39m=\u001b[39m XSoftmax\u001b[39m.\u001b[39;49mapply(attention_scores, attention_mask, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    671\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtalking_head:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/autograd/function.py:534\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines a rule for the behavior of this autograd.Function underneath\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m    :func:`torch.vmap`. For a :func:`torch.autograd.Function` to support\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m    :func:`torch.vmap`, you must either override this staticmethod, or set\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39m    Please see :ref:`func-autograd-function` for more details.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    530\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use autograd.Function with vmap, you must either override the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    531\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvmap staticmethod or set generate_vmap_rule=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m     )\n\u001b[0;32m--> 534\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m         \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#两两之间的bertscore相似度\n",
    "import statistics\n",
    "import json\n",
    "qwen1_5_files = ['xsum_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'newsroom_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'cnndm_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'bbc2024_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama2_files = ['xsum_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'newsroom_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'cnndm_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'bbc2024_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama3_files = ['xsum_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl']\n",
    "\n",
    "qwen2_file = ['xsum_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl']\n",
    "\n",
    "\n",
    "phi3_files = ['xsum_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'newsroom_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'cnndm_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'bbc2024_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl']\n",
    "\n",
    "qwen2_7b_files = ['xsum_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'newsroom_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'cnndm_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'bbc2024_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl']\n",
    "\n",
    "mistral_ins_files = ['xsum_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'newsroom_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'cnndm_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'bbc2024_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl']\n",
    "\n",
    "chatglm3_files = ['xsum_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'newsroom_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'cnndm_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl']\n",
    "\n",
    "phi2_files = ['bbc2024_llama2_no_limit_len_pretrained__microsoft__phi-2__trust_remote_code__True.jsonl',\n",
    "              'bbc2024_llama2_no_limit_len_pretrained__microsoft__phi-2__trust_remote_code__True.jsonl',\n",
    "              'bbc2024_llama2_no_limit_len_pretrained__microsoft__phi-2__trust_remote_code__True.jsonl',\n",
    "              'bbc2024_llama2_no_limit_len_pretrained__microsoft__phi-2__trust_remote_code__True.jsonl']\n",
    "\n",
    "qwen2_7b_ins = ['']\n",
    "\n",
    "base_chatglm3_path = './qwen1_5/chatglm3'\n",
    "\n",
    "base_mistral_ins_path = './qwen1_5/mistralai_Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "base_qwen2_7b__path = './qwen1_5/Qwen_Qwen2-7B'\n",
    "\n",
    "base_phi3 = './qwen1_5/microsoft_Phi-3-mini-4k-instruct_new_version/'\n",
    "\n",
    "base_qwen_path = './qwen1_5/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama2_path = './llama2/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama3_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama3_70b_summary'\n",
    "\n",
    "base_qwen2_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen2_72b_summary'\n",
    "\n",
    "base_phi2 = './llama2/microsoft_phi-2'\n",
    "\n",
    "total_score = 0\n",
    "for i in range(len(qwen1_5_files)):\n",
    "    name = qwen1_5_files[i].split('_')[0]\n",
    "    \n",
    "    # if(name!='bbc2024'):\n",
    "    #     continue\n",
    "    print(name)\n",
    "    \n",
    "    f1 = base_qwen_path + '/' + qwen1_5_files[i]\n",
    "    f2 = base_llama2_path + '/' + llama2_files[i]\n",
    "    f3 = base_llama3_path + '/' + llama3_files[i]\n",
    "    #use json to load f1 and f2\n",
    "    qwen = json.load(open(f1))\n",
    "    llama2 = json.load(open(f2))\n",
    "    \n",
    "    llama3 = json.load(open(f3))\n",
    "    qwen2  = json.load(open(base_qwen2_path + '/' + qwen2_file[i]))\n",
    "    phi3 = json.load(open(base_phi3 + '/' + phi3_files[i]))\n",
    "    qwen2_7b = json.load(open(base_qwen2_7b__path + '/' + qwen2_7b_files[i]))\n",
    "    mistral_ins = json.load(open(base_mistral_ins_path + '/' + mistral_ins_files[i]))\n",
    "    chatglm3 = json.load(open(base_chatglm3_path + '/' + chatglm3_files[i]))\n",
    "    \n",
    "    phi2 = json.load(open(base_phi2 + '/' + phi2_files[i]))\n",
    "    \n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "  \n",
    "    list_score = []\n",
    "    list_len = []\n",
    "    all_len = 0 \n",
    "    for j in range(len(qwen)):\n",
    "        # print(llama2[j])\n",
    "        # score+= BertScore(chatglm3[j][\"filtered_resps\"], phi3[j][\"filtered_resps\"][0])['f1'][0]\n",
    "        # score+= BertScore(qwen2_7b[j][\"filtered_resps\"][0], phi3[j][\"filtered_resps\"][0])['f1'][0]\n",
    "        tmp_score = BertScore(llama3[j][\"qwen_reference_summary\"],qwen[j][\"target\"])['f1'][0]\n",
    "        # score+= BertScore(phi3[j][\"filtered_resps\"][0], qwen2[j][\"qwen_reference_summary\"])['f1'][0]\n",
    "        # tmp_score = BertScore(qwen2_7b[j][\"filtered_resps\"][0], llama2[j]['target'])['f1'][0]*100\n",
    "        # tmp_len = len(phi2[j][\"filtered_resps\"][0].split(' '))\n",
    "      \n",
    "        \n",
    "        list_score.append(tmp_score)\n",
    "        # list_len.append(tmp_len)\n",
    "        score+= tmp_score\n",
    "        # all_len+=tmp_len\n",
    "\n",
    "    print(f'min score {min(list_score)*100}')\n",
    "    print(f'max score {max(list_score)*100}')\n",
    "\n",
    "    # print(statistics.stdev(list_score))\n",
    "    print(score/len(qwen)*100)\n",
    "    # print(f'min len {min(list_len)}')\n",
    "    # print(f'max len {max(list_len)}')\n",
    "\n",
    "    # print(statistics.stdev(list_len))\n",
    "    # print(all_len/len(qwen))\n",
    "    # # total_score+=score/len(qwen)\n",
    "# print(\"total \")\n",
    "# print((total_score/4)*100)\n",
    "    \n",
    "    \n",
    "  #phi3\n",
    "  #72.80, 73.47, 74.85, 100, 74.02, 74.51, 74.01, 72.52\n",
    "  \n",
    "# qwen2_72b vs qwen1.5_72B   76.36 74.39 76.68 75.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min length 23\n",
      "max length 466\n",
      "average length 75.01\n",
      "40.3684669792216\n",
      "bertscore\n",
      "min score 52.377837896347046\n",
      "max score 96.03741765022278\n",
      "average score 71.84401156902314\n",
      "5.631091019004214\n"
     ]
    }
   ],
   "source": [
    "#calculate stderr\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "# f = './llama2/microsoft_Phi-3-mini-4k-instruct_new_version/bbc2024_llama2_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl'\n",
    "# f = './qwen1_5/Qwen_Qwen2-1.5B/bbc2024_qwen_no_limit_len_pretrained__Qwen__Qwen2-1.5B__trust_remote_code__True.jsonl'\n",
    "# f = './llama2/google_gemma-1.1-2b-it/bbc2024_llama2_no_limit_len_pretrained__google__gemma-1.1-2b-it__trust_remote_code__True.jsonl'\n",
    "# f = './llama2/mistralai_Mistral-7B-Instruct-v0.2/bbc2024_llama2_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl'\n",
    "# f = './llama2/Qwen_Qwen2-7B-Instruct/bbc2024_llama2_no_limit_len_pretrained__Qwen__Qwen2-7B-Instruct__trust_remote_code__True.jsonl'\n",
    "# f = './llama2/chatglm3/bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl_chatglm3.jsonl'\n",
    "# f = './llama2/microsoft_phi-2/bbc2024_llama2_no_limit_len_pretrained__microsoft__phi-2__trust_remote_code__True.jsonl'\n",
    "# f = 'qwen1_5/microsoft_Phi-3-mini-4k-instruct_new_version/bbc2024_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl'\n",
    "# f = './llama2/chatglm3/bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl_chatglm3.jsonl'\n",
    "\n",
    "# f = './factKB/Qwen2-1.5B-Instruct/factkb_2_sentences_no_system_prompt_bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit_Qwen2-1.5B-Instruct.jsonl'\n",
    "# f = './diff_prompt/qwen2-1.5b/bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit_qwen2-1.5b.jsonl'\n",
    "f = './diff_prompt/Qwen2-1.5B-Instruct/diff_words_no_system_prompt_bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit_Qwen2-1.5B-Instruct.jsonl'\n",
    "data = json.load(open(f))\n",
    "\n",
    "score_list = []\n",
    "\n",
    "bert_list = []\n",
    "for i in range(len(data)):\n",
    "    score = len(data[i]['filtered_resps1'].split(' '))\n",
    "    if(score>500):\n",
    "        print(data[i]['doc_id'])\n",
    "    score_list.append(score)\n",
    "    \n",
    "    bert_list.append(data[i]['bertscore_f1_1'])\n",
    "    \n",
    "\n",
    "print(f'min length {min(score_list)}')\n",
    "print(f'max length {max(score_list)}')\n",
    "print(f'average length {sum(score_list)/len(score_list)}')\n",
    "# print(np.std(score_list))\n",
    "print(statistics.stdev(score_list))\n",
    "\n",
    "\n",
    "print('bertscore')\n",
    "print(f'min score {min(bert_list)}')\n",
    "print(f'max score {max(bert_list)}')\n",
    "print(f'average score {sum(bert_list)/len(bert_list)}')\n",
    "# print(np.std(score_list))\n",
    "print(statistics.stdev(bert_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 21:40:38.645470: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-16 21:40:38.727662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 21:40:39.892656: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a88257a3204d43b2cebfb85a0a56c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Manchester City's Phil Foden scored two crucial goals against Manchester United, securing a victory and moving closer to a Champions League spot. The match, which ended 3-1, highlighted Foden's maturity and versatility, with City now just one point behind Liverpool in the Premier League title race.\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "import transformers\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "class MultiTokenEOSCriteria(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        initial_decoder_input_length: int,\n",
    "        batch_size: int,\n",
    "    ) -> None:\n",
    "        self.initial_decoder_input_length = initial_decoder_input_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.sequence = sequence\n",
    "        self.sequence_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        # print(sequence, self.sequence_ids)\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = len(self.sequence_ids) + 2\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "        lookback_ids_batch = input_ids[:, self.initial_decoder_input_length :]\n",
    "\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        lookback_tokens_batch = self.tokenizer.batch_decode(lookback_ids_batch)\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                self.done_tracker[i] = self.sequence in lookback_tokens_batch[i]\n",
    "        return False not in self.done_tracker\n",
    "\n",
    "\n",
    "def stop_sequences_criteria(\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    stop_sequences: list[str],\n",
    "    initial_decoder_input_length: int,\n",
    "    batch_size: int,\n",
    ") -> transformers.StoppingCriteriaList:\n",
    "    return transformers.StoppingCriteriaList(\n",
    "        [\n",
    "            *[\n",
    "                MultiTokenEOSCriteria(\n",
    "                    sequence, tokenizer, initial_decoder_input_length, batch_size\n",
    "                )\n",
    "                for sequence in stop_sequences\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "news = \"Last updated on .From the section Premier League Pep Guardiola hailed Phil Foden as the \\\"best player in the Premier League right now\\\" after his second-half double ensured Manchester City came from behind to beat Manchester United at Etihad Stadium. In a game packed with international stars, it was two local boys who provided the key moments. The visitors had led through Marcus Rashford's sensational eighth-minute strike from 30 yards. However, in a contest City dominated, Foden took centre stage. There was an element of controversy over his second-half equaliser. United boss Erik ten Hag was booked for arguing his side should have had a free-kick in the City half when Rashford went down under Kyle Walker's challenge. Contact was minimal and within seconds Foden had curled a superb shot past Andre Onana. Foden then burst away from a static Casemiro to score City's second from Julian Alvarez's return pass before Erling Haaland rounded off the scoring in stoppage time, after the Norwegian earlier missed an open goal from barely three yards. \\\"It is the amount of games he is playing,\\\" Guardiola told Match of the Day about Foden. \\\"He was always a talented player but now he is more mature and understands more the game, especially defensively. He can play middle, right, make moments and cut inside, play in the left, scoring from the left. \\\"What can I say? He is the best player in the Premier League right now for the amount of things he does. Unbelievable.\\\" It was City's sixth win in seven meetings against United, who suffered their 11th Premier League defeat of the season. More importantly, the result means Pep Guardiola's side move to within a point of leaders Liverpool before next week's trip to Anfield. United remain in sixth in the table but are now 11 points behind fourth-placed Aston Villa and six behind Tottenham, in what could turn out to be a fifth Champions League spot, having played a game more. • None Premier League title race run-in: Will Liverpool, Manchester City or Arsenal come out on top? There was something fitting about Stockport-born Foden becoming the match winner. It seems a long time ago now that Guardiola was having to defend his treatment of a player he was accused of be holding back while contemporary Jadon Sancho was excelling at Borussia Dortmund having decided he would not wait to develop at City. The contrast in current fortunes for the two players could not be more marked, with United outcast Sancho back on loan at Dortmund. Foden's equaliser was City's 600th goal in this stadium under Guardiola and was a sublime effort, curled into the top corner out of Onana's reach after he had run across Victor Lindelof on the right edge of the United box. His second saw him burst into the area from the other side, collect a pass from Julian Alvarez and drill a low shot home, almost through the United keeper, who might have done better. Foden is now picking his moments to get involved in attacks and watching England boss Gareth Southgate must surely find a way of getting the 23-year-old, who has scored 18 goals this season, into his starting line-up at Euro 2024 alongside the likes of Harry Kane and Jude Bellingham. Haaland will have been pleased to get his name on the scoresheet given his incredible first-half miss when he somehow put Foden's knock-back over an open goal from three yards. \\\"He was disappointed, I was disappointed. I want him to score four or five goals every game like he did against Luton [in the FA Cup],\\\" said Guardiola. \\\"But I don't care. He can miss this one, it is the reaction. He is sad for 10 seconds and he can miss five more and is sad for 10 seconds in his mind but after that erase it and on to the next one. \\\"The great, great players I met, and I've been fortunate as a player and especially a manager, they have this incredible ability to forget in an instant. \\\"Tennis players, golf players, basketball players, when they miss, and everyone misses, they say 'OK', smile, be positive and go for the next. That defines the great players and he did it.\\\" • None How did you rate Manchester City's performance? Have your say here • None What did you make of Manchester United's display? Send us your views here Rashford on target but United well beaten In the build-up to the game, Rashford spoke extensively to Players' Tribune, detailing his journey through poverty to United's first team and underlining how much the club means to him. It was thought-provoking stuff and very personal. Yet, for a sizeable number of fans who have viewed Rashford's efforts this season through the prism of underperformance, a lack of goal threat and a body language of indifference, the words meant nothing. For them the general reaction was 'save it for the pitch'. To that end, it is quite possible Rashford will never score a better goal than his eighth-minute opener. Bruno Fernandes did well to hold off Ruben Dias to control Onana's booming kick downfield. Then, as Scott McTominay sprinted into the area, Fernandes had the vision and intelligence to wait for Rashford coming behind. He rolled a pass perfectly to the England striker, who did not have to break stride as he launched a shot into the City net off the underside of the bar. Rashford could easily have had another but, in attempting to control a bouncing ball as Fernandes lifted a pass towards him beyond a static home defence, he succeeded only in heading it into the ground, which allowed Kyle Walker to get back and snuff out the danger. That turned out to be United's last realistic opportunity as they attempted to repel wave after wave of City attacks. Ten Hag simply did not have the personnel to complete the task, as hard as his players tried. New co-owner Sir Jim Ratcliffe has said he wants to knock City off their perch. On this evidence, he is going to need a pretty long ladder to attempt that. \"\n",
    " \n",
    "prompt = f\"News: {news}\\nSummarize the news in two sentences. Summary:\"\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": prompt}, \n",
    "   \n",
    "] \n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 256, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False,\n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n",
    "print(len(output[0]['generated_text'].split(' ')))\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda:0')\n",
    "\n",
    "# stopping_criteria = stop_sequences_criteria(tokenizer, ['\\n', '</s>'], inputs['input_ids'].shape[1], 1)\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=2000, stopping_criteria=stopping_criteria, do_sample=False)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406290432\n"
     ]
    }
   ],
   "source": [
    "#pegasus\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "torch.set_default_device(\"cuda:0\")\n",
    "\n",
    "\n",
    "doc = 'President Joe Biden’s mission to speed up the lackluster rollout of coronavirus vaccines. Biden signed a slew of executive orders this week aimed at accelerating the distribution of vaccines.'\n",
    "\n",
    "model_name = 'Yale-LILY/brio-cnndm-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))\n",
    "# with torch.no_grad():\n",
    "#     batch = tokenizer(doc, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "#     preds = model.generate(**batch)\n",
    "#     tgt_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "#     print(tgt_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester City defeated Manchester United 2-1 in a Premier League match, with Phil Foden scoring twice and helping City secure its sixth win in seven meetings.\n"
     ]
    }
   ],
   "source": [
    "#qwen2 ins\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda:1\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    \n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "\n",
    "news = \"Last updated on .From the section Premier League Pep Guardiola hailed Phil Foden as the \\\"best player in the Premier League right now\\\" after his second-half double ensured Manchester City came from behind to beat Manchester United at Etihad Stadium. In a game packed with international stars, it was two local boys who provided the key moments. The visitors had led through Marcus Rashford's sensational eighth-minute strike from 30 yards. However, in a contest City dominated, Foden took centre stage. There was an element of controversy over his second-half equaliser. United boss Erik ten Hag was booked for arguing his side should have had a free-kick in the City half when Rashford went down under Kyle Walker's challenge. Contact was minimal and within seconds Foden had curled a superb shot past Andre Onana. Foden then burst away from a static Casemiro to score City's second from Julian Alvarez's return pass before Erling Haaland rounded off the scoring in stoppage time, after the Norwegian earlier missed an open goal from barely three yards. \\\"It is the amount of games he is playing,\\\" Guardiola told Match of the Day about Foden. \\\"He was always a talented player but now he is more mature and understands more the game, especially defensively. He can play middle, right, make moments and cut inside, play in the left, scoring from the left. \\\"What can I say? He is the best player in the Premier League right now for the amount of things he does. Unbelievable.\\\" It was City's sixth win in seven meetings against United, who suffered their 11th Premier League defeat of the season. More importantly, the result means Pep Guardiola's side move to within a point of leaders Liverpool before next week's trip to Anfield. United remain in sixth in the table but are now 11 points behind fourth-placed Aston Villa and six behind Tottenham, in what could turn out to be a fifth Champions League spot, having played a game more. • None Premier League title race run-in: Will Liverpool, Manchester City or Arsenal come out on top? There was something fitting about Stockport-born Foden becoming the match winner. It seems a long time ago now that Guardiola was having to defend his treatment of a player he was accused of be holding back while contemporary Jadon Sancho was excelling at Borussia Dortmund having decided he would not wait to develop at City. The contrast in current fortunes for the two players could not be more marked, with United outcast Sancho back on loan at Dortmund. Foden's equaliser was City's 600th goal in this stadium under Guardiola and was a sublime effort, curled into the top corner out of Onana's reach after he had run across Victor Lindelof on the right edge of the United box. His second saw him burst into the area from the other side, collect a pass from Julian Alvarez and drill a low shot home, almost through the United keeper, who might have done better. Foden is now picking his moments to get involved in attacks and watching England boss Gareth Southgate must surely find a way of getting the 23-year-old, who has scored 18 goals this season, into his starting line-up at Euro 2024 alongside the likes of Harry Kane and Jude Bellingham. Haaland will have been pleased to get his name on the scoresheet given his incredible first-half miss when he somehow put Foden's knock-back over an open goal from three yards. \\\"He was disappointed, I was disappointed. I want him to score four or five goals every game like he did against Luton [in the FA Cup],\\\" said Guardiola. \\\"But I don't care. He can miss this one, it is the reaction. He is sad for 10 seconds and he can miss five more and is sad for 10 seconds in his mind but after that erase it and on to the next one. \\\"The great, great players I met, and I've been fortunate as a player and especially a manager, they have this incredible ability to forget in an instant. \\\"Tennis players, golf players, basketball players, when they miss, and everyone misses, they say 'OK', smile, be positive and go for the next. That defines the great players and he did it.\\\" • None How did you rate Manchester City's performance? Have your say here • None What did you make of Manchester United's display? Send us your views here Rashford on target but United well beaten In the build-up to the game, Rashford spoke extensively to Players' Tribune, detailing his journey through poverty to United's first team and underlining how much the club means to him. It was thought-provoking stuff and very personal. Yet, for a sizeable number of fans who have viewed Rashford's efforts this season through the prism of underperformance, a lack of goal threat and a body language of indifference, the words meant nothing. For them the general reaction was 'save it for the pitch'. To that end, it is quite possible Rashford will never score a better goal than his eighth-minute opener. Bruno Fernandes did well to hold off Ruben Dias to control Onana's booming kick downfield. Then, as Scott McTominay sprinted into the area, Fernandes had the vision and intelligence to wait for Rashford coming behind. He rolled a pass perfectly to the England striker, who did not have to break stride as he launched a shot into the City net off the underside of the bar. Rashford could easily have had another but, in attempting to control a bouncing ball as Fernandes lifted a pass towards him beyond a static home defence, he succeeded only in heading it into the ground, which allowed Kyle Walker to get back and snuff out the danger. That turned out to be United's last realistic opportunity as they attempted to repel wave after wave of City attacks. Ten Hag simply did not have the personnel to complete the task, as hard as his players tried. New co-owner Sir Jim Ratcliffe has said he wants to knock City off their perch. On this evidence, he is going to need a pretty long ladder to attempt that. \"\n",
    "prompt = f'News: {news}\\nSummarize the news in two sentences. Summary: '\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful summary assistant. You can help users summarize news in two sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)\n",
    "#Manchester City defeated Manchester United 3-1 with a crucial double from Phil Foden and a hat-trick from Erling Haaland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n",
      "/home/xbr/anaconda3/envs/test/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/xbr/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-small-8k-instruct/69caae1f2acea34b26f535fecb1f2abb9a304695/triton_flash_blocksparse_attn.py:88: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  x = [xi.to_sparse_csr() for xi in x]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 2x + 3 = 7, follow these steps:\n",
      "\n",
      "1. Subtract 3 from both sides of the equation: 2x + 3 - 3 = 7 - 3\n",
      "2. Simplify: 2x = 4\n",
      "3. Divide both sides by 2: 2x/2 = 4/2\n",
      "4. Simplify: x = 2\n",
      "\n",
      "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model_id = \"microsoft/Phi-3-small-8k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "assert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\n",
    "device = \"cuda:5\"\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,trust_remote_code=True)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629cd9ebd7114bb2b256acde796adaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Duke of Sussex can go ahead with claims against Associated Newspapers of unlawfully obtaining information, as a court ruling opens the way for a trial. The Daily Mail and Mail on Sunday publishers wanted to stop the case, arguing claims of getting \"information by\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,pipeline,AutoModelForCausalLM\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "model_name = 'microsoft/phi-2'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = model.eval()\n",
    "\n",
    "messages = \"News: The Duke of Sussex can go ahead with claims against Associated Newspapers of unlawfully obtaining information, as a court ruling opens the way for a trial. The Daily Mail and Mail on Sunday publishers wanted to stop the case, arguing claims of getting \\\"information by deception\\\" were out of time. But a judge has decided the case, involving Prince Harry and six other high-profile claimants, can proceed. Associated Newspapers strongly denied the allegations as \\\"preposterous\\\". In a High Court ruling on Friday, Mr Justice Nicklin said Associated Newspapers had \\\"not been able to deliver a 'knockout blow' to the claims of any of these claimants\\\". As well as Prince Harry, the newspaper group faces multiple claims of \\\"gross breaches of privacy\\\" from Sir Elton John, David Furnish, Elizabeth Hurley, Sadie Frost, Sir Simon Hughes and Baroness Doreen Lawrence. Sir Elton John was among the claimants welcoming the judge's ruling This includes allegations of bugging devices in cars, listening into phone calls and dishonestly obtaining medical and financial information. \\\"We are delighted with today's decision which allows our claims over serious criminal activity and gross breaches of privacy by the Mail titles to proceed to trial,\\\" said a joint statement from the claimants. But a statement from Associated Newspapers said: \\\"As we have always made unequivocally clear, the lurid claims made by Prince Harry and others of phone-hacking, landline-tapping, burglary and sticky-window microphones are simply preposterous and we look forward to establishing this in court in due course.\\\" The newspaper group also welcomed as a \\\"significant victory\\\" the ruling that unpublished information given on \\\"strict grounds of confidentiality\\\" to the Leveson Inquiry into phone hacking could not be used as evidence in this case. Prince Harry had made a surprise court appearance when the case was initially heard in March Prince Harry, in this latest battle with the UK's tabloid press, made a surprise appearance at an earlier hearing of the the case against Associated Newspapers, at the High Court in London in March. The Daily Mail and Mail on Sunday publishers had categorically denied the allegations and said the claims had \\\"no real prospects of succeeding\\\". But their lawyers had also argued that in any event the allegations were outside the requirement to bring claims within six years. Some of the allegations are from decades ago, but lawyers for Prince Harry and the claimants successfully argued that new evidence had come to light and they were unaware at the time of how information was being covertly acquired. Mr Justice Nicklin's ruling accepted that the claimants might not have known about such \\\"concealed\\\" gathering of information about them. \\\"In my judgment, the claimants have a real prospect of demonstrating not only that the unlawful acts themselves were concealed, but also, in many instances, further devices were employed in the published articles to throw the subject 'off the scent',\\\" said the judge's ruling. \\\"Several claimants complain that they believed that their confidences were being betrayed by people close to them.\\\" Baroness Doreen Lawrence also attended an earlier court hearing about her allegations The judge's decision was welcomed by actor Hugh Grant, the director of the Hacked Off group, which campaigns for press reforms. \\\"This ruling is a significant blow to the Daily Mail and great news for anyone who wants the truth about allegations of illegal press practices to come out,\\\" he said. Privacy lawyer Philippa Dempster, of the law firm Freeths, said the ruling that the claims can go ahead despite the passage of time would \\\"send a shockwave across the press industry\\\". The ruling could also mean another in-person court appearance from Prince Harry, who earlier this year stepped into the witness box to give evidence in a hacking claim against another newspaper publisher, Mirror Group Newspapers. He became the first senior royal in modern times to make such a court appearance, facing questions over two days, with the outcome of that case still to be decided. You can see more royal stories in the free BBC Royal Watch newsletter emailed each week - sign up here from within the UK. or here, from outside the UK.\\nSummarize the news in two sentences. Summary:\"\n",
    "# model_inputs = tokenizer(messages, return_tensors=\"pt\",return_attention_mask=False).to(device)\n",
    "model_inputs = tokenizer(messages, return_tensors=\"pt\", return_attention_mask=False).to(device)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=False)\n",
    "# output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n",
    "\n",
    "generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eed711da2a47e8be6b66d8383bcd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 2779683840\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-1.1-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 统计参数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total trainable parameters: {count_parameters(model)}')\n",
    "\n",
    "# input_text = \"Which is bigger? 9.9 or 9.11\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda:7\")\n",
    "\n",
    "# outputs = model.generate(**input_ids,max_new_tokens=20)\n",
    "\n",
    "\n",
    "# generated_ids = [\n",
    "#         output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids.input_ids, outputs)\n",
    "#     ]\n",
    "\n",
    "\n",
    "# output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
