{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "bert_score = load(\"bertscore\")\n",
    "def BertScore(refs, preds):\n",
    "    bert_score_res = bert_score.compute(predictions=[preds], references=[refs], model_type=\"microsoft/deberta-xlarge-mnli\", lang=\"en\")\n",
    "    \n",
    "    return bert_score_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.8450139760971069], 'recall': [0.5974714756011963], 'f1': [0.7000024914741516], 'hashcode': 'microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.12(hug_trans=4.39.3)'}\n"
     ]
    }
   ],
   "source": [
    "a = 'Manchester City defeated Manchester United 3-1 with a double from Phil Foden and a goal from Erling Haaland.'\n",
    "b = 'Manchester City came from behind to beat Manchester United 3-1 in a thrilling Premier League match. Phil Foden scored a brace, including a stunning equalizer, and Erling Haaland added a third goal for City in stoppage time. Marcus Rashford had given United an early lead with a sensational strike, but City dominated the game and deservedly took all three points. The win moves City to within a point of leaders Liverpool, while United remain in sixth place.'\n",
    "print(BertScore(b,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n",
      "0.7546372020244598\n",
      "newsroom\n",
      "0.7415115525722503\n",
      "cnndm\n",
      "0.759126748085022\n",
      "bbc2024\n",
      "0.7524805088043213\n"
     ]
    }
   ],
   "source": [
    "qwen1_5_files = ['xsum_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'newsroom_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'cnndm_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'bbc2024_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama2_files = ['xsum_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'newsroom_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'cnndm_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'bbc2024_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama3_files = ['xsum_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl']\n",
    "\n",
    "base_qwen_path = './qwen1_5/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama2_path = './llama2/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama3_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama3_70b_summary/'\n",
    "\n",
    "for i in range(len(qwen1_5_files)):\n",
    "    name = qwen1_5_files[i].split('_')[0]\n",
    "    print(name)\n",
    "    \n",
    "    f1 = base_qwen_path + '/' + qwen1_5_files[i]\n",
    "    f2 = base_llama2_path + '/' + llama2_files[i]\n",
    "    f3 = base_llama3_path + '/' + llama3_files[i]\n",
    "    #use json to load f1 and f2\n",
    "    qwen = json.load(open(f1))\n",
    "    llama2 = json.load(open(f2))\n",
    "    \n",
    "    llama3 = json.load(open(f3))\n",
    "    \n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for j in range(len(qwen)):\n",
    "        score+= BertScore(llama2[j]['target'], llama3[j][\"qwen_reference_summary\"])['f1'][0]\n",
    "        \n",
    "    print(score/len(qwen))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be1445cccbf4c9ebfff697b930c4392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 399/500 [09:33<02:25,  1.44s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb 单元格 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m news \u001b[39m=\u001b[39m llama2[j][\u001b[39m'\u001b[39m\u001b[39marticle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m reference \u001b[39m=\u001b[39m llama2[j][\u001b[39m'\u001b[39m\u001b[39mqwen_reference_summary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m response, history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mchat(tokenizer, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNews: \u001b[39;49m\u001b[39m{\u001b[39;49;00mnews\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mSummarize the news in two sentences. Summary:\u001b[39;49m\u001b[39m'\u001b[39;49m, history\u001b[39m=\u001b[39;49m[],do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m response \u001b[39m=\u001b[39m clean_summary(response)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a227862725f613130305f325f4e5553227d/home/xbr/LLM/lm-evaluation-harness/acl/evaluate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#f1 bertscore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:1042\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.chat\u001b[0;34m(self, tokenizer, query, history, role, max_length, num_beams, do_sample, top_p, temperature, logits_processor, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1040\u001b[0m eos_token_id \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39meos_token_id, tokenizer\u001b[39m.\u001b[39mget_command(\u001b[39m\"\u001b[39m\u001b[39m<|user|>\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1041\u001b[0m                 tokenizer\u001b[39m.\u001b[39mget_command(\u001b[39m\"\u001b[39m\u001b[39m<|observation|>\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m-> 1042\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgen_kwargs, eos_token_id\u001b[39m=\u001b[39;49meos_token_id)\n\u001b[1;32m   1043\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m][\u001b[39mlen\u001b[39m(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]):\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   1044\u001b[0m response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1527\u001b[0m         input_ids,\n\u001b[1;32m   1528\u001b[0m         candidate_generator\u001b[39m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1541\u001b[0m     )\n\u001b[1;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1543\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1544\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1545\u001b[0m         input_ids,\n\u001b[1;32m   1546\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1547\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1548\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1549\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1550\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1551\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1552\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1553\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1554\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1555\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1558\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2403\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2404\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2405\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2406\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2407\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2408\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2409\u001b[0m )\n\u001b[1;32m   2411\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2412\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:941\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, return_last_logit)\u001b[0m\n\u001b[1;32m    938\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[1;32m    939\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 941\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    942\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    943\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    944\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    945\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    946\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    947\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    948\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    949\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    950\u001b[0m )\n\u001b[1;32m    952\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    953\u001b[0m \u001b[39mif\u001b[39;00m return_last_logit:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:834\u001b[0m, in \u001b[0;36mChatGLMModel.forward\u001b[0;34m(self, input_ids, position_ids, attention_mask, full_attention_mask, past_key_values, inputs_embeds, use_cache, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    831\u001b[0m rotary_pos_emb \u001b[39m=\u001b[39m rotary_pos_emb\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    833\u001b[0m \u001b[39m# Run encoder.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m hidden_states, presents, all_hidden_states, all_self_attentions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    835\u001b[0m     inputs_embeds, full_attention_mask, rotary_pos_emb\u001b[39m=\u001b[39;49mrotary_pos_emb,\n\u001b[1;32m    836\u001b[0m     kv_caches\u001b[39m=\u001b[39;49mpast_key_values, use_cache\u001b[39m=\u001b[39;49muse_cache, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    840\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(v \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m [hidden_states, presents, all_hidden_states, all_self_attentions] \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:641\u001b[0m, in \u001b[0;36mGLMTransformer.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_caches, use_cache, output_hidden_states)\u001b[0m\n\u001b[1;32m    631\u001b[0m     layer_ret \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    632\u001b[0m         layer,\n\u001b[1;32m    633\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m         use_reentrant\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     )\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     layer_ret \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    642\u001b[0m         hidden_states,\n\u001b[1;32m    643\u001b[0m         attention_mask,\n\u001b[1;32m    644\u001b[0m         rotary_pos_emb,\n\u001b[1;32m    645\u001b[0m         kv_cache\u001b[39m=\u001b[39;49mkv_caches[index],\n\u001b[1;32m    646\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m hidden_states, kv_cache \u001b[39m=\u001b[39m layer_ret\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:544\u001b[0m, in \u001b[0;36mGLMBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    542\u001b[0m layernorm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    543\u001b[0m \u001b[39m# Self attention.\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m attention_output, kv_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(\n\u001b[1;32m    545\u001b[0m     layernorm_output,\n\u001b[1;32m    546\u001b[0m     attention_mask,\n\u001b[1;32m    547\u001b[0m     rotary_pos_emb,\n\u001b[1;32m    548\u001b[0m     kv_cache\u001b[39m=\u001b[39;49mkv_cache,\n\u001b[1;32m    549\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache\n\u001b[1;32m    550\u001b[0m )\n\u001b[1;32m    552\u001b[0m \u001b[39m# Residual connection.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_residual_connection_post_layernorm:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/THUDM/chatglm3-6b/6f3b58ec10f088978ae174398f9d20b6dfc71552/modeling_chatglm.py:408\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, rotary_pos_emb, kv_cache, use_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m# apply relative positional encoding (rotary embedding)\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m rotary_pos_emb \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     query_layer \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n\u001b[1;32m    409\u001b[0m     key_layer \u001b[39m=\u001b[39m apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n\u001b[1;32m    411\u001b[0m \u001b[39m# adjust key and value for inference\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate ChatGLM3\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "def clean_summary(completion):\n",
    "        completion = completion[2:] if completion.startswith(\"\\n\\n\") else completion\n",
    "        completion = completion.replace('The news summary is: \\\"', '')\n",
    "        completion_list = completion.split('\\n')\n",
    "        if(len(completion_list)<3):\n",
    "            completion = completion_list[-1]\n",
    "        else:\n",
    "            completion = completion.split('\\n')[2]\n",
    "        candidate = completion\n",
    "        complete_sentences = re.findall(r'[^.!?]*[.!?]', completion)\n",
    "        # 将匹配到的完整句子连接成一个新的字符串\n",
    "        completion = ''.join(complete_sentences)\n",
    "        if(completion==''):\n",
    "            completion = candidate\n",
    "        return completion\n",
    "    \n",
    "    \n",
    "qwen1_5_files = ['xsum_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl']\n",
    "base_qwen1_5_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen72b_summary'\n",
    "\n",
    "\n",
    "llama2_files = ['xsum_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama2_70b_summary_no_len_limit.jsonl']\n",
    "base_llama2_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama2_70b_summary'\n",
    " \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "average_result = dict()\n",
    "for i in range(len(llama2_files)):\n",
    "    name = llama2_files[i].split('_')[0]\n",
    "    print(name)\n",
    "    f1 = base_llama2_path + '/' + llama2_files[i]\n",
    "    #use json to load f1 \n",
    "    llama2 = json.load(open(f1))\n",
    "    \n",
    "    \n",
    "    result_list = list()\n",
    "    result_name = llama2_files[i] + '_chatglm3.jsonl'\n",
    "\n",
    "    average_score = 0\n",
    "    for j in tqdm(range(len(llama2))):\n",
    "        news = llama2[j]['article']\n",
    "        reference = llama2[j]['qwen_reference_summary']\n",
    "        \n",
    "        response, history = model.chat(tokenizer, f'News: {news}\\nSummarize the news in two sentences. Summary:', history=[],do_sample=False)\n",
    "        response = clean_summary(response)\n",
    "        \n",
    "        #f1 bertscore\n",
    "        score = BertScore(reference, response)['f1'][0]\n",
    "        average_score += score\n",
    "        \n",
    "        tmp_dict = dict()\n",
    "        tmp_dict['article'] = news\n",
    "        tmp_dict['target'] = reference\n",
    "        tmp_dict['filtered_resps'] = response\n",
    "        tmp_dict['bertscore_f1'] = score\n",
    "        \n",
    "        result_list.append(tmp_dict)\n",
    "         \n",
    "        \n",
    "    average_result[name] = average_score/len(llama2)\n",
    "    \n",
    "    #save result to jsonl\n",
    "    with open('./llama2/chatglm3/'+result_name, 'w') as f:\n",
    "        json.dump(result_list, f, indent=4)\n",
    "    with open('./llama2/chatglm3/average.jsonl', 'w') as f:\n",
    "        json.dump([average_result], f, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725672722876072\n"
     ]
    }
   ],
   "source": [
    "file = './qwen1_5/chatglm3/bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl'\n",
    "\n",
    "data = json.load(open(file))\n",
    "\n",
    "score = 0\n",
    "for i in data:\n",
    "    score += i['bertscore_f1']\n",
    "print(score/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum\n",
      "0.7305820472240449\n",
      "newsroom\n",
      "0.710476155757904\n",
      "cnndm\n",
      "0.7347142081260681\n",
      "bbc2024\n",
      "0.7250446029901505\n",
      "total \n",
      "72.5204253524542\n"
     ]
    }
   ],
   "source": [
    "#两两之间的bertscore相似度\n",
    "\n",
    "qwen1_5_files = ['xsum_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'newsroom_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'cnndm_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                 'bbc2024_qwen_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama2_files = ['xsum_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'newsroom_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'cnndm_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl',\n",
    "                'bbc2024_llama2_no_limit_len_pretrained__ahxt__LiteLlama-460M-1T__trust_remote_code__True.jsonl']\n",
    "\n",
    "llama3_files = ['xsum_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_llama3_70b_summary_no_len_limit.jsonl']\n",
    "\n",
    "qwen2_file = ['xsum_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'newsroom_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'cnndm_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl',\n",
    "                'bbc2024_sample_500_0k5_1k5_qwen2_summary_no_len_limit.jsonl']\n",
    "\n",
    "\n",
    "phi3_files = ['xsum_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'newsroom_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'cnndm_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl',\n",
    "              'bbc2024_qwen_no_limit_len_pretrained__microsoft__Phi-3-mini-4k-instruct__trust_remote_code__True.jsonl']\n",
    "\n",
    "qwen2_7b_files = ['xsum_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'newsroom_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'cnndm_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl',\n",
    "            'bbc2024_qwen_no_limit_len_pretrained__Qwen__Qwen2-7B__trust_remote_code__True.jsonl']\n",
    "\n",
    "mistral_ins_files = ['xsum_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'newsroom_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'cnndm_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl',\n",
    "                    'bbc2024_qwen_no_limit_len_pretrained__mistralai__Mistral-7B-Instruct-v0.2__trust_remote_code__True.jsonl']\n",
    "\n",
    "chatglm3_files = ['xsum_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'newsroom_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'cnndm_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl',\n",
    "           'bbc2024_sample_500_0k5_1k5_qwen1.5_72b_summary_no_len_limit.jsonl_chatglm3.jsonl']\n",
    "\n",
    "\n",
    "base_chatglm3_path = './qwen1_5/chatglm3'\n",
    "\n",
    "base_mistral_ins_path = './qwen1_5/mistralai_Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "base_qwen2_7b__path = './qwen1_5/Qwen_Qwen2-7B'\n",
    "\n",
    "base_phi3 = './qwen1_5/microsoft_Phi-3-mini-4k-instruct_new_version/'\n",
    "\n",
    "base_qwen_path = './qwen1_5/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama2_path = './llama2/ahxt_LiteLlama-460M-1T'\n",
    "\n",
    "base_llama3_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_llama3_70b_summary'\n",
    "\n",
    "base_qwen2_path = '/home/xbr/LLM/summary_benchmark/dataset/sample_500_qwen2_72b_summary'\n",
    "\n",
    "\n",
    "total_score = 0\n",
    "for i in range(len(qwen1_5_files)):\n",
    "    name = qwen1_5_files[i].split('_')[0]\n",
    "    print(name)\n",
    "    \n",
    "    f1 = base_qwen_path + '/' + qwen1_5_files[i]\n",
    "    f2 = base_llama2_path + '/' + llama2_files[i]\n",
    "    f3 = base_llama3_path + '/' + llama3_files[i]\n",
    "    #use json to load f1 and f2\n",
    "    qwen = json.load(open(f1))\n",
    "    llama2 = json.load(open(f2))\n",
    "    \n",
    "    llama3 = json.load(open(f3))\n",
    "    qwen2  = json.load(open(base_qwen2_path + '/' + qwen2_file[i]))\n",
    "    phi3 = json.load(open(base_phi3 + '/' + phi3_files[i]))\n",
    "    qwen2_7b = json.load(open(base_qwen2_7b__path + '/' + qwen2_7b_files[i]))\n",
    "    mistral_ins = json.load(open(base_mistral_ins_path + '/' + mistral_ins_files[i]))\n",
    "    chatglm3 = json.load(open(base_chatglm3_path + '/' + chatglm3_files[i]))\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for j in range(len(qwen)):\n",
    "        # score+= BertScore(chatglm3[j][\"filtered_resps\"], phi3[j][\"filtered_resps\"][0])['f1'][0]\n",
    "        # score+= BertScore(qwen2_7b[j][\"filtered_resps\"][0], phi3[j][\"filtered_resps\"][0])['f1'][0]\n",
    "        # score+= BertScore(qwen2[j][\"qwen_reference_summary\"],chatglm3[j]['filtered_resps'])['f1'][0]\n",
    "         score+= BertScore(phi3[j][\"filtered_resps\"][0], qwen2[j][\"qwen_reference_summary\"])['f1'][0]\n",
    "        # score+= BertScore(phi3[j][\"filtered_resps\"][0], qwen[j]['target'])['f1'][0]\n",
    "        \n",
    "    print(score/len(qwen))\n",
    "    total_score+=score/len(qwen)\n",
    "print(\"total \")\n",
    "print((total_score/4)*100)\n",
    "    \n",
    "    \n",
    "  #phi3\n",
    "  #72.80, 73.47, 74.85, 100, 74.02, 74.51, 74.01, 72.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abc1614c75a427bbe4b6a36416b4cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/xbr/anaconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Manchester City's Phil Foden was hailed as the best player in the Premier League by Pep Guardiola after scoring two crucial goals against Manchester United, helping City secure a 3-1 victory at Etihad Stadium. The win moves City within a point of Liverpool for the title race, while United's performance highlighted the need for improvement under Erik ten Hag's management.\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "import transformers\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "\n",
    "class MultiTokenEOSCriteria(transformers.StoppingCriteria):\n",
    "    \"\"\"Criteria to stop on the specified multi-token sequence.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        initial_decoder_input_length: int,\n",
    "        batch_size: int,\n",
    "    ) -> None:\n",
    "        self.initial_decoder_input_length = initial_decoder_input_length\n",
    "        self.done_tracker = [False] * batch_size\n",
    "        self.sequence = sequence\n",
    "        self.sequence_ids = tokenizer.encode(sequence, add_special_tokens=False)\n",
    "        # print(sequence, self.sequence_ids)\n",
    "        # we look back for 2 more tokens than it takes to encode our stop sequence\n",
    "        # because tokenizers suck, and a model might generate `['\\n', '\\n']` but our `sequence` is `['\\n\\n']`\n",
    "        # and we don't want to mistakenly not stop a generation because our\n",
    "        # (string) stop sequence was output in a different tokenization\n",
    "\n",
    "        # NOTE: there is a minor danger that this will end up looking back 2 tokens into the past, into the inputs to the model,\n",
    "        # and stopping generation immediately as a result. With only 2 extra tokens of lookback, this risk is minimized\n",
    "        # Additionally, in lookback_ids_batch we should prevent ever looking back into the inputs as described.\n",
    "        self.sequence_id_len = len(self.sequence_ids) + 2\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs) -> bool:\n",
    "        # For efficiency, we compare the last n tokens where n is the number of tokens in the stop_sequence\n",
    "        lookback_ids_batch = input_ids[:, self.initial_decoder_input_length :]\n",
    "\n",
    "        lookback_ids_batch = lookback_ids_batch[:, -self.sequence_id_len :]\n",
    "\n",
    "        lookback_tokens_batch = self.tokenizer.batch_decode(lookback_ids_batch)\n",
    "\n",
    "        for i, done in enumerate(self.done_tracker):\n",
    "            if not done:\n",
    "                self.done_tracker[i] = self.sequence in lookback_tokens_batch[i]\n",
    "        return False not in self.done_tracker\n",
    "\n",
    "\n",
    "def stop_sequences_criteria(\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    stop_sequences: list[str],\n",
    "    initial_decoder_input_length: int,\n",
    "    batch_size: int,\n",
    ") -> transformers.StoppingCriteriaList:\n",
    "    return transformers.StoppingCriteriaList(\n",
    "        [\n",
    "            *[\n",
    "                MultiTokenEOSCriteria(\n",
    "                    sequence, tokenizer, initial_decoder_input_length, batch_size\n",
    "                )\n",
    "                for sequence in stop_sequences\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
    "prompt = \"News: Last updated on .From the section Premier League Pep Guardiola hailed Phil Foden as the \\\"best player in the Premier League right now\\\" after his second-half double ensured Manchester City came from behind to beat Manchester United at Etihad Stadium. In a game packed with international stars, it was two local boys who provided the key moments. The visitors had led through Marcus Rashford's sensational eighth-minute strike from 30 yards. However, in a contest City dominated, Foden took centre stage. There was an element of controversy over his second-half equaliser. United boss Erik ten Hag was booked for arguing his side should have had a free-kick in the City half when Rashford went down under Kyle Walker's challenge. Contact was minimal and within seconds Foden had curled a superb shot past Andre Onana. Foden then burst away from a static Casemiro to score City's second from Julian Alvarez's return pass before Erling Haaland rounded off the scoring in stoppage time, after the Norwegian earlier missed an open goal from barely three yards. \\\"It is the amount of games he is playing,\\\" Guardiola told Match of the Day about Foden. \\\"He was always a talented player but now he is more mature and understands more the game, especially defensively. He can play middle, right, make moments and cut inside, play in the left, scoring from the left. \\\"What can I say? He is the best player in the Premier League right now for the amount of things he does. Unbelievable.\\\" It was City's sixth win in seven meetings against United, who suffered their 11th Premier League defeat of the season. More importantly, the result means Pep Guardiola's side move to within a point of leaders Liverpool before next week's trip to Anfield. United remain in sixth in the table but are now 11 points behind fourth-placed Aston Villa and six behind Tottenham, in what could turn out to be a fifth Champions League spot, having played a game more. • None Premier League title race run-in: Will Liverpool, Manchester City or Arsenal come out on top? There was something fitting about Stockport-born Foden becoming the match winner. It seems a long time ago now that Guardiola was having to defend his treatment of a player he was accused of be holding back while contemporary Jadon Sancho was excelling at Borussia Dortmund having decided he would not wait to develop at City. The contrast in current fortunes for the two players could not be more marked, with United outcast Sancho back on loan at Dortmund. Foden's equaliser was City's 600th goal in this stadium under Guardiola and was a sublime effort, curled into the top corner out of Onana's reach after he had run across Victor Lindelof on the right edge of the United box. His second saw him burst into the area from the other side, collect a pass from Julian Alvarez and drill a low shot home, almost through the United keeper, who might have done better. Foden is now picking his moments to get involved in attacks and watching England boss Gareth Southgate must surely find a way of getting the 23-year-old, who has scored 18 goals this season, into his starting line-up at Euro 2024 alongside the likes of Harry Kane and Jude Bellingham. Haaland will have been pleased to get his name on the scoresheet given his incredible first-half miss when he somehow put Foden's knock-back over an open goal from three yards. \\\"He was disappointed, I was disappointed. I want him to score four or five goals every game like he did against Luton [in the FA Cup],\\\" said Guardiola. \\\"But I don't care. He can miss this one, it is the reaction. He is sad for 10 seconds and he can miss five more and is sad for 10 seconds in his mind but after that erase it and on to the next one. \\\"The great, great players I met, and I've been fortunate as a player and especially a manager, they have this incredible ability to forget in an instant. \\\"Tennis players, golf players, basketball players, when they miss, and everyone misses, they say 'OK', smile, be positive and go for the next. That defines the great players and he did it.\\\" • None How did you rate Manchester City's performance? Have your say here • None What did you make of Manchester United's display? Send us your views here Rashford on target but United well beaten In the build-up to the game, Rashford spoke extensively to Players' Tribune, detailing his journey through poverty to United's first team and underlining how much the club means to him. It was thought-provoking stuff and very personal. Yet, for a sizeable number of fans who have viewed Rashford's efforts this season through the prism of underperformance, a lack of goal threat and a body language of indifference, the words meant nothing. For them the general reaction was 'save it for the pitch'. To that end, it is quite possible Rashford will never score a better goal than his eighth-minute opener. Bruno Fernandes did well to hold off Ruben Dias to control Onana's booming kick downfield. Then, as Scott McTominay sprinted into the area, Fernandes had the vision and intelligence to wait for Rashford coming behind. He rolled a pass perfectly to the England striker, who did not have to break stride as he launched a shot into the City net off the underside of the bar. Rashford could easily have had another but, in attempting to control a bouncing ball as Fernandes lifted a pass towards him beyond a static home defence, he succeeded only in heading it into the ground, which allowed Kyle Walker to get back and snuff out the danger. That turned out to be United's last realistic opportunity as they attempted to repel wave after wave of City attacks. Ten Hag simply did not have the personnel to complete the task, as hard as his players tried. New co-owner Sir Jim Ratcliffe has said he wants to knock City off their perch. On this evidence, he is going to need a pretty long ladder to attempt that. • None Attempt missed. Casemiro (Manchester United) header from the centre of the box misses to the right. Assisted by Bruno Fernandes following a set piece situation. • None Substitution, Manchester City. Oscar Bobb replaces Phil Foden because of an injury. • None Goal! Manchester City 3, Manchester United 1. Erling Haaland (Manchester City) left footed shot from the right side of the box to the bottom left corner. Assisted by Rodri. • None Attempt missed. Kevin De Bruyne (Manchester City) right footed shot from outside the box is close, but misses to the right. Assisted by Phil Foden. • None Rodri (Manchester City) wins a free kick on the right wing. • None Goal! Manchester City 2, Manchester United 1. Phil Foden (Manchester City) left footed shot from the left side of the box to the bottom right corner. Assisted by Julián Álvarez. Navigate to the next page Navigate to the last page\\nSummarize the news in two sentences. Summary:\"\n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "    {\"role\": \"user\", \"content\": prompt}, \n",
    "   \n",
    "] \n",
    "\n",
    "\n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 256, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False,\n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])\n",
    "print(len(output[0]['generated_text'].split(' ')))\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to('cuda:0')\n",
    "\n",
    "# stopping_criteria = stop_sequences_criteria(tokenizer, ['\\n', '</s>'], inputs['input_ids'].shape[1], 1)\n",
    "\n",
    "# outputs = model.generate(**inputs, max_length=2000, stopping_criteria=stopping_criteria, do_sample=False)\n",
    "# text = tokenizer.batch_decode(outputs)[0]\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden signed a slew of executive orders this week aimed at accelerating the distribution of vaccines.\n"
     ]
    }
   ],
   "source": [
    "#pegasus\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "torch.set_default_device(\"cuda:0\")\n",
    "\n",
    "\n",
    "doc = 'President Joe Biden’s mission to speed up the lackluster rollout of coronavirus vaccines. Biden signed a slew of executive orders this week aimed at accelerating the distribution of vaccines.'\n",
    "\n",
    "model_name = 'google/pegasus-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = tokenizer(doc, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "    preds = model.generate(**batch)\n",
    "    tgt_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    print(tgt_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester City defeated Manchester United 3-1 with a double from Phil Foden and a goal from Erling Haaland.\n"
     ]
    }
   ],
   "source": [
    "#qwen2 ins\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "\n",
    "\n",
    "news = \"Last updated on .From the section Premier League Pep Guardiola hailed Phil Foden as the \\\"best player in the Premier League right now\\\" after his second-half double ensured Manchester City came from behind to beat Manchester United at Etihad Stadium. In a game packed with international stars, it was two local boys who provided the key moments. The visitors had led through Marcus Rashford's sensational eighth-minute strike from 30 yards. However, in a contest City dominated, Foden took centre stage. There was an element of controversy over his second-half equaliser. United boss Erik ten Hag was booked for arguing his side should have had a free-kick in the City half when Rashford went down under Kyle Walker's challenge. Contact was minimal and within seconds Foden had curled a superb shot past Andre Onana. Foden then burst away from a static Casemiro to score City's second from Julian Alvarez's return pass before Erling Haaland rounded off the scoring in stoppage time, after the Norwegian earlier missed an open goal from barely three yards. \\\"It is the amount of games he is playing,\\\" Guardiola told Match of the Day about Foden. \\\"He was always a talented player but now he is more mature and understands more the game, especially defensively. He can play middle, right, make moments and cut inside, play in the left, scoring from the left. \\\"What can I say? He is the best player in the Premier League right now for the amount of things he does. Unbelievable.\\\" It was City's sixth win in seven meetings against United, who suffered their 11th Premier League defeat of the season. More importantly, the result means Pep Guardiola's side move to within a point of leaders Liverpool before next week's trip to Anfield. United remain in sixth in the table but are now 11 points behind fourth-placed Aston Villa and six behind Tottenham, in what could turn out to be a fifth Champions League spot, having played a game more. • None Premier League title race run-in: Will Liverpool, Manchester City or Arsenal come out on top? There was something fitting about Stockport-born Foden becoming the match winner. It seems a long time ago now that Guardiola was having to defend his treatment of a player he was accused of be holding back while contemporary Jadon Sancho was excelling at Borussia Dortmund having decided he would not wait to develop at City. The contrast in current fortunes for the two players could not be more marked, with United outcast Sancho back on loan at Dortmund. Foden's equaliser was City's 600th goal in this stadium under Guardiola and was a sublime effort, curled into the top corner out of Onana's reach after he had run across Victor Lindelof on the right edge of the United box. His second saw him burst into the area from the other side, collect a pass from Julian Alvarez and drill a low shot home, almost through the United keeper, who might have done better. Foden is now picking his moments to get involved in attacks and watching England boss Gareth Southgate must surely find a way of getting the 23-year-old, who has scored 18 goals this season, into his starting line-up at Euro 2024 alongside the likes of Harry Kane and Jude Bellingham. Haaland will have been pleased to get his name on the scoresheet given his incredible first-half miss when he somehow put Foden's knock-back over an open goal from three yards. \\\"He was disappointed, I was disappointed. I want him to score four or five goals every game like he did against Luton [in the FA Cup],\\\" said Guardiola. \\\"But I don't care. He can miss this one, it is the reaction. He is sad for 10 seconds and he can miss five more and is sad for 10 seconds in his mind but after that erase it and on to the next one. \\\"The great, great players I met, and I've been fortunate as a player and especially a manager, they have this incredible ability to forget in an instant. \\\"Tennis players, golf players, basketball players, when they miss, and everyone misses, they say 'OK', smile, be positive and go for the next. That defines the great players and he did it.\\\" • None How did you rate Manchester City's performance? Have your say here • None What did you make of Manchester United's display? Send us your views here Rashford on target but United well beaten In the build-up to the game, Rashford spoke extensively to Players' Tribune, detailing his journey through poverty to United's first team and underlining how much the club means to him. It was thought-provoking stuff and very personal. Yet, for a sizeable number of fans who have viewed Rashford's efforts this season through the prism of underperformance, a lack of goal threat and a body language of indifference, the words meant nothing. For them the general reaction was 'save it for the pitch'. To that end, it is quite possible Rashford will never score a better goal than his eighth-minute opener. Bruno Fernandes did well to hold off Ruben Dias to control Onana's booming kick downfield. Then, as Scott McTominay sprinted into the area, Fernandes had the vision and intelligence to wait for Rashford coming behind. He rolled a pass perfectly to the England striker, who did not have to break stride as he launched a shot into the City net off the underside of the bar. Rashford could easily have had another but, in attempting to control a bouncing ball as Fernandes lifted a pass towards him beyond a static home defence, he succeeded only in heading it into the ground, which allowed Kyle Walker to get back and snuff out the danger. That turned out to be United's last realistic opportunity as they attempted to repel wave after wave of City attacks. Ten Hag simply did not have the personnel to complete the task, as hard as his players tried. New co-owner Sir Jim Ratcliffe has said he wants to knock City off their perch. On this evidence, he is going to need a pretty long ladder to attempt that. • None Attempt missed. Casemiro (Manchester United) header from the centre of the box misses to the right. Assisted by Bruno Fernandes following a set piece situation. • None Substitution, Manchester City. Oscar Bobb replaces Phil Foden because of an injury. • None Goal! Manchester City 3, Manchester United 1. Erling Haaland (Manchester City) left footed shot from the right side of the box to the bottom left corner. Assisted by Rodri. • None Attempt missed. Kevin De Bruyne (Manchester City) right footed shot from outside the box is close, but misses to the right. Assisted by Phil Foden. • None Rodri (Manchester City) wins a free kick on the right wing. • None Goal! Manchester City 2, Manchester United 1. Phil Foden (Manchester City) left footed shot from the left side of the box to the bottom right corner. Assisted by Julián Álvarez. Navigate to the next page Navigate to the last page\"\n",
    "prompt = f'News: {news}\\nSummarize the news in two sentences. Summary: '\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful summary assistant. You can help users summarize news in two sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)\n",
    "#Manchester City defeated Manchester United 3-1 with a crucial double from Phil Foden and a hat-trick from Erling Haaland."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
